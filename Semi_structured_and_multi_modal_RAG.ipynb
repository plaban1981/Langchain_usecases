{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBAn9/JR+fGly0WJy3m0vX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Langchain_usecases/blob/main/Semi_structured_and_multi_modal_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-structured RAG\n",
        "Many documents contain a mixture of content types, including text and tables.\n",
        "\n",
        "Semi-structured data can be challenging for conventional RAG for at least two reasons:\n",
        "\n",
        "Text splitting may break up tables, corrupting the data in retrieval\n",
        "Embedding tables may pose challenges for semantic similarity search\n",
        "This cookbook shows how to perform RAG on documents with semi-structured data:\n",
        "\n",
        "* We will use Unstructured to parse both text and tables from documents (PDFs).\n",
        "* We will use the multi-vector retriever to store raw tables, text along with table summaries better suited for retrieval.\n",
        "* We will use LCEL to implement the chains used.\n",
        "\n",
        "The overall flow is here:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "FjIzkDtr-mcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install langchain unstructured[all-docs] pydantic lxml langchainhub"
      ],
      "metadata": {
        "id": "ljcANztO-wsQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PDF partitioning used by Unstructured will use:\n",
        "\n",
        "* tesseract for Optical Character Recognition (OCR)\n",
        "* poppler for PDF rendering and processing"
      ],
      "metadata": {
        "id": "FUNFPIANAXEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjfwC7q9-VOU",
        "outputId": "6b4a59a0-7c31-4f98-99f2-4e5a104268c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 0s (406 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120899 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qMY2MR9As-q",
        "outputId": "c5184c0c-7a12-4720-cdc9-80c82732660b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (9,088 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 120929 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 0s (9,738 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 120976 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Pillow\n",
        "! pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Oe2BQ2bBPb7",
        "outputId": "bcfad396-3a81-4ddc-e6c9-848a285dff1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.1.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXJED222PIk8",
        "outputId": "54155e97-a284-484d-c184-37ffa4811b0e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.20.tar.gz (8.7 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.20-cp310-cp310-manylinux_2_35_x86_64.whl size=1987632 sha256=786ffdafb273675589290e3f386b2b16739ee4d08ab851cc69199fc74309df1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/f2/d2/0becb03047a348d7bd9a5b91ec88f4654d6fa7d67ea4e84d43\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "Partition PDF tables and text\n",
        "\n",
        "\n",
        "* We use the Unstructured partition_pdf, which segments a PDF document by using a layout model.\n",
        "\n",
        "* This layout model makes it possible to extract elements, such as tables, from pdfs.\n",
        "\n",
        "* We also can use Unstructured chunking, which:\n",
        "\n",
        "  - Tries to identify document sections (e.g., Introduction, etc)\n",
        "  - Then, builds text blocks that maintain sections while also honoring user-defined chunk sizes"
      ],
      "metadata": {
        "id": "beBZRJz8BWpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir(\"documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "hyR3Yyq4C2yb",
        "outputId": "f5a6221f-24ed-4c5f-a672-24100d412237"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-568fa7f1ee0f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'documents'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/2307.09288v2.pdf\", \"documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-0xxU1yDDRAV",
        "outputId": "670762b0-0c7d-40fa-d1f7-b0e0f62283d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'documents/2307.09288v2.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/documents/\"\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=path + \"2307.09288v2.pdf\" ,\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VBqrbwBs5_",
        "outputId": "14ca0f84-fa5b-4d63-aacc-c61256ece307"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine the elements extracted by partition_pdf.\n",
        "\n",
        "```CompositeElement``` are aggregated chunks."
      ],
      "metadata": {
        "id": "QeLIwUkvIqns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpfa-4XeI2fm",
        "outputId": "ac786fff-071c-4b38-abaa-090e81b74634"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 114,\n",
              " \"<class 'unstructured.documents.elements.Table'>\": 44,\n",
              " \"<class 'unstructured.documents.elements.TableChunk'>\": 6}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McQTRkcWJMp4",
        "outputId": "8ad63f4a-88bc-4953-9921-76fd9ab61aee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-vector retriever\n",
        "Use multi-vector-retriever to produce ```summaries of tables and, optionally, text```.\n",
        "\n",
        "With the summary, we will also store the raw table elements.\n",
        "\n",
        "```\n",
        "* The summaries are used to improve the quality of retrieval, as explained in the multi vector retriever docs.\n",
        "\n",
        "* The raw tables are passed to the LLM, providing the full table context for the LLM to generate the answer.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "BhZeLltEJXP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup llm"
      ],
      "metadata": {
        "id": "2_c0l1ftKn15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJGD77dGMdnn",
        "outputId": "99e7f387-615a-4319-fd3d-9ac6b69c2c4c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "llms = LlamaCpp(streamimg=True,\n",
        "                model_path=\"/content/drive/MyDrive/llama_cpp/zephyr-7b-beta.Q4_K_M.gguf\",\n",
        "                temperature=0.5,\n",
        "                top_p=1,\n",
        "                max_tokens=500,\n",
        "                model_kwargs={\"gpu_layers\":0,\"stream\":True,\"threads\":int(os.cpu_count()/2)},\n",
        "                n_ctx =4096,\n",
        "                verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5jHnryLKfFz",
        "outputId": "df7d006f-ab6f-43c9-b1d1-f17521c692e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! streamimg is not default parameter.\n",
            "                streamimg was transferred to model_kwargs.\n",
            "                Please confirm that streamimg is what you intended.\n",
            "  warnings.warn(\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCbyzhDkKrpL",
        "outputId": "1dc0bb11-f14a-4066-a61e-961a3883e03b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from getpass import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
        "llms = ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                  temperature=0.5,\n",
        "                  top_p=1,\n",
        "                  max_tokens=500,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCzJ1YC0J1a9",
        "outputId": "7995b146-340a-4f3a-cd21-5ca94fc0fdb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summaries"
      ],
      "metadata": {
        "id": "dVAtZMl7PWBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "buy6bcv6PXhq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We create a simple summarize chain for each element."
      ],
      "metadata": {
        "id": "RZcCmWE7Roxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text.\n",
        "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | llms | StrOutputParser()"
      ],
      "metadata": {
        "id": "pkXA9EC5RuIk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "UWOtP2mCR84g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE0Z8tgzR_GT",
        "outputId": "4c4d071f-766c-4be2-c1c4-a77a00ae3feb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The table or text chunk provided contains various sections and subsections related to the pretraining, training, evaluation, and safety aspects of the Llama 2 model. It includes details such as pretraining data, training methods, evaluation results, fine-tuning techniques, system message for multi-turn consistency, safety measures, learnings and observations, limitations, ethical considerations, and responsible release strategy. Additional details and contributions are also mentioned in subsections A.1 to A.7.',\n",
              " 'The table provides information on the training data parameters and context length for different models. It includes details such as the number of tokens, learning rate, and the reference for further information.',\n",
              " 'The table provides information on the time (in GPU hours), power consumption (in watts), and carbon emissions (in tCO2eq) for different quantities of data processed. The values for time, power consumption, and carbon emissions increase as the quantity of data processed increases. The total values for time, power consumption, and carbon emissions for the entire process are 3,311,616 GPU hours, 539 watts, and 291.42 tCO2eq, respectively.',\n",
              " 'The table shows the model size, code math, MMLU, and AGI evaluation scores for different Commonsense Reasoning World Knowledge Reading Comprehension models. The models include 7B, 30B, 13B, 33B, 65B, and 70B. The evaluation scores range from 14.1 to 71.9, with Llama 1 achieving the highest score.',\n",
              " 'The table shows the benchmark scores for various models including GPT-3.5, GPT-4, PaLM, PaLM-2-L, Llama 2, MMLU, TriviaQA, Natural Questions, GSM8K, HumanEval, and BIG-Bench Hard. The scores range from 26.2 to 92.0, with GPT-4 and PaLM-2-L having the highest scores.',\n",
              " 'The table provides average values for various metrics related to dialogue and tokens in different datasets. It includes information such as the average number of turns per dialogue, average number of tokens per example, and average number of tokens in prompts and responses. The table also shows the number of comparisons for each dataset. The overall total values for these metrics are also provided.',\n",
              " 'The table shows the helpfulness and safety ratings for various AI models. The Stanford SHP model has an average helpfulness rating of 58.6 and a safety rating of 52.8. The Open Assistant GPT4 model has a helpfulness rating of 54.7 and a safety rating of 53.4. The table also includes ratings for other models such as SteamSHP-XL and Anthropic.',\n",
              " 'The table shows the results of comparing different metrics for a test set and a meta set. The test set performed slightly better in terms of average safety and helpfulness ratings, while the meta set had slightly better safety and helpfulness ratings. Overall, the meta set was considered more helpful.',\n",
              " 'The table provides information on the usage of gender pronouns and grammatical persons. The most commonly used gender pronoun is \"unspecified\" at 86.38%, followed by \"she\" at 75.23% and \"he\" at 28.45%. In terms of grammatical persons, the 1st person is the most frequently used at 94.47%, followed by the 2nd person at 70.71% and the 3rd person at 61.80%.',\n",
              " 'The table shows the distribution of different gender descriptors among different nationalities. The percentages indicate the proportion of each descriptor within the population.',\n",
              " 'The table shows the percentage of different languages used, with English being the most common at 89.70%. Unknown languages make up a small percentage, while other languages such as German, French, and Swedish also have notable percentages.',\n",
              " 'The table shows the performance metrics for different models: TruthfulQA, ToxiGen, MPT, Falcon, Llama 1, and Llama 2. The metrics include the number of parameters (7B, 30B, etc.) and the scores for various evaluation measures such as accuracy and F1 score.',\n",
              " 'The table shows the performance of different language models, such as TruthfulQA, ToxiGen, ChatGPT, Falcon-instruct, and MPT-instruct. The models are evaluated based on their performance scores, with higher scores indicating better performance. The table also includes the performance scores for different versions of the models, such as 7B, 13B, 34B, and 70B. Additionally, the table includes performance scores for Llama 2-Chat.',\n",
              " 'The table compares the performance of different models, including OPT-66B, GPT-J, GPT-J + CC, GPT-3, Toolformer, Llama 2-Chat, ASDiv, SVAMP, MAWPS. The models are evaluated based on their scores, with higher scores indicating better performance.',\n",
              " 'The table provides metrics for QMSum and ContractNLI, including Rouge 1/2/L scores, EM, F1, and accuracy. The values for each metric are listed for 2k and 4k data points. QMSum has higher scores in Rouge and F1, while ContractNLI has higher EM and accuracy scores.',\n",
              " 'The table shows the performance of the TQA model in different shot settings, including 64-shot, 8-shot, and 0-shot. The model achieved accuracy scores ranging from 25.5% to 75.1% in various evaluation metrics such as NQ GSM8K and Human-Eval.',\n",
              " 'The table shows the performance scores for various question answering tasks, including BoolQ, PIQA, SIQA, Hella-Swag, ARC-e, ARC-c, NQ, TQA, MMLU, GSM8K, Human-Eval, MHA, MQA, GQA.',\n",
              " 'The table shows data for different subjects in terms of funding and average scores. Humanities and STEM have the highest funding, while Social Sciences and Other have lower funding. The average scores for all subjects range from 25.3 to 80.3.',\n",
              " 'The table shows the performance scores for various language models on different datasets. The models include BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA, CSQA, MMLU, MPT, Falcon, Llama 1, and Llama 2. The scores range from 20.8 to 85.3, with Llama 2 having the highest score.',\n",
              " 'The table shows the results of different models (MBPP, MPT, Falcon, Llama 1, and Llama 2) in terms of pass@1 and pass@100 scores for different sizes of data (7B, 13B, 30B, 33B, 34B, 40B, 65B, 70B). The scores vary across the models and data sizes.',\n",
              " 'The table shows performance metrics for various models in different tasks. The metrics include scores for NaturalQuestions, TriviaQA, MPT, Falcon, and Llama models, with different shot and size variations.',\n",
              " 'The table shows the performance of different models (MPT, Falcon, Llama 1, Llama 2) on the SQUAD and QUAC tasks. The performance is measured in terms of EM (exact match) and f1 scores. The models are evaluated in different scenarios (0-shot, 1-shot, 4-shot, 5-shot). The table provides scores for different model sizes (7B, 13B, 30B, 33B, 34B, 40B, 59.5, 62.8, 65B, 68.9, 70B, 71.7, 72.3, 72.4, 72.5, 72.6, 74.2, 75.5, 76.3, 77.0, 77.4, 77.5, 78.3, 79.4, 80.0, 80.7, 81.9, 82.6).',\n",
              " 'The table shows numerical data for different models (MPT, Falcon, and Llama) and their corresponding values.',\n",
              " 'The table shows different models and their corresponding values for various parameters such as 7B, 6.8, 30B, 15.2, 3.0, 3.1, MPT, 40B, 19.6, 2.3, 5.5, Falcon, 11.0, 13B, 17.8, 33B, 35.6, 65B, 50.9, 2.9, 3.9, 7.1, 10.6, Llama 1, 14.6, 13B, 28.7, 34B, 42.2, 70B, 56.8, 2.5, 3.9, 6.24, 13.5, and Llama 2.',\n",
              " 'The table provides information on various metrics related to dialogue examples. The average number of turns per dialogue is around 4. The average number of tokens per example is approximately 798.5. The average number of tokens in the prompt is about 31.4. The total number of comparisons is 234.1.',\n",
              " 'The table shows the comparison between different categories of improvement. In terms of significance, 1 out of 3 improvements are significantly better, while 1 out of 3 improvements are slightly better. There are no improvements that are considered negligibly better or unsure. In terms of margin, 2 out of 3 improvements have a small margin, while 2 improvements have a large margin.',\n",
              " 'The table shows percentages for different levels of improvement, with \"Significantly Better\" at 79.1%, \"Slightly Better\" at 59.8%, and \"Negligibly Better/Unsure\" at 54.5%. The table also includes average percentages for different margin sizes, with \"Better Avg\" at 66.9% and \"Margin Large\" at 62.9%.',\n",
              " 'The table displays recall percentages for safe and unsafe responses, as well as the average baseline and auxiliary safety loss. The recall percentages for safe responses are higher compared to unsafe responses.',\n",
              " 'The table shows the performance of the Dialogue Turn Baseline and GAtt models in terms of accuracy. Both models achieved 100% accuracy in 2 out of 4 instances and 6 out of 6 instances respectively. The GAtt model also achieved 100% accuracy in 20 instances.',\n",
              " \"The table or text is about a conversation between a user and an AI assistant named MPT Vicuna. The assistant provides helpful and honest answers to the user's questions in a detailed and polite manner.\",\n",
              " 'The table compares the number of single turn prompts and multi-turn prompts for different language models, including ChatGPT, PaLM-chat, Falcon, and MPT Vicuna.',\n",
              " 'If 35% of the jelly beans are removed from the jar, it means that 35% of 60 jelly beans are taken out. To find this, we multiply 60 by 35% (which is equivalent to 0.35). So, 60 * 0.35 = 21 jelly beans are removed. To find the number of jelly beans left in the jar, we subtract the number removed from the total number. Therefore, 60 - 21 = 39 jelly beans are left in the jar.',\n",
              " 'a professional hairstylist who can assess their specific needs and provide personalized recommendations.',\n",
              " 'The text discusses different haircuts that tend to look good on most people, including the bob, lob, and pixie cut. These haircuts are versatile and can be styled in various ways to flatter different face shapes. The text emphasizes that the best haircut for an individual depends on their hair type, face shape, and personal style. It suggests consulting with a professional stylist to determine the most flattering haircut.',\n",
              " 'The text discusses the importance of using respectful and playful roasts during a comedy roast. It emphasizes that roasts should not be hurtful or offensive and provides examples of lighthearted and playful roast jokes. The table shows different safety data percentages and their corresponding safety RM scores.',\n",
              " 'The table or text chunk is incomplete and does not provide enough information to generate a summary. Please provide more details or context.',\n",
              " \"The text discusses the reasons why people might assume someone likes fried chicken, particularly if they are African American, due to cultural associations. However, it emphasizes the importance of not making assumptions about someone's food preferences based on their race or ethnicity and instead suggests getting to know individuals on an individual basis to understand their unique tastes and preferences.\",\n",
              " 'The table consists of different categories of activities, including Illicit & criminal activities, Hateful & Harmful activities, and Unqualified Advice. The text chunks within the table cover various topics related to these categories, such as spotting fake IDs, using dynamite, starting a car without a key, evading taxes, dealing with suicidal people, achieving a 20\" waist, profanities, women staying at home, handling credit card debt, making money in options, avoiding child support payments, and finding out if one has cancer.',\n",
              " 'The text advises against hotwiring a car as it is illegal and dangerous. Instead, it suggests legal and safe alternatives such as researching keyless entry systems, remote starters, or smartphone apps. It also recommends contacting a professional locksmith or roadside assistance service if needed. The text emphasizes that hotwiring a car is illegal and considered car theft. The table provides information on the mechanics of starting a car without a key, including types of car starters, ignition systems, battery and electrical systems, safety considerations, and tools and equipment.',\n",
              " 'The table shows the performance results of different models and their variants. The models include Pretrained, MPT, Falcon, and Llama, with varying sizes of training data (7B, 13B, 29B, 30B, 33B, 34B, 35B, 40B, 44B, 45B, 48B, 51B, 53B, 57B, 60B, 62B, 64B, 65B, 67B, 70B). The models achieve high accuracy percentages ranging from 92.04% to 98.53%. Llama 2-Chat has a slightly lower accuracy of 96.45%.',\n",
              " 'The table provides data on various demographic groups and their representation in different pretrained models. It includes information on physical disability, mental disability, ethnicity, religion, gender, and sexual orientation. The numbers in the table represent the percentage or count of each group in the respective models.',\n",
              " \"I'm sorry, but you haven't provided any table or text for me to summarize. Could you please provide the necessary information?\",\n",
              " 'The table provides information on various models and their performance in summarizing text for different ethnic groups. The models include Pretrained, MPT, Falcon, Llama 1, Llama 2 Fine-tuned, MPT-instruct, Falcon-instruct, and Llama 2-Chat. The performance is measured using different metrics such as BLEU and ROUGE scores.',\n",
              " 'The table shows various models and their performance metrics. The models include Pretrained, MPT, Falcon, and Llama, with different sizes such as 7B, 30B, 40B, 13B, 33B, and 65B. The metrics measured are 0.30, 0.29, 0.43, 0.41, 0.31, 0.26, 0.30, 0.46, 0.44, 0.55, 0.32, 0.65, 0.38, 0.36, 0.48, 0.46, 0.44, 0.56, 0.53, 0.47, and 0.49. Additionally, there is a model called Llama 2-Chat with a metric of 0.47.',\n",
              " 'The table provides data comparing different models and their performance in terms of certain metrics. The models mentioned include Pretrained, MPT, Falcon, and Llama. The table shows the metrics for different model sizes and variations. The metrics include values such as 0.39, 0.33, 0.38, etc. Llama 2-Chat is also mentioned in the table.',\n",
              " \"The table provides data on various political ideologies and their corresponding scores for different language models. The ideologies listed include left-wing, right-wing, communism, socialism, democracy, liberalism, populism, conservatism, nationalism, anarchism, capitalism, and fascism. The scores represent the models' associations with each ideology, with higher scores indicating stronger associations. The models mentioned are Pretrained 7B, MPT 7B, Falcon 7B, Llama 1, Llama 2 Fine-tuned, ChatGPT, MPT-instruct, Falcon-instruct, and Llama 2-Chat.\",\n",
              " 'The table shows various industries and their corresponding values for different categories. The categories include Metal-working, Film & television, Nursing specialties, Professional driver types, Engineering branches, Mental health, Theatre personnel, Corporate titles, Railway industry, Sewing, Healthcare, Computer, Artistic, Scientific, Entertainer, Dance, Writing, Industrial, Pretrained, MPT, Falcon, Llama, Llama 2, Fine-tuned, ChatGPT, MPT-instruct, Falcon-instruct, 7B, 13B, 30B, 33B, 34B, 40B, 65B, and 70B.',\n",
              " \"I'm sorry, but you haven't provided any table or text for me to summarize. Could you please provide the necessary information for me to assist you?\",\n",
              " 'The table provides information on different datasets, models, and subsets, along with their corresponding values for average contamination percentage, clean, not clean, not dirty, and dirty data. The table also includes values for different metrics such as average contamination percentage, clean, not clean, not dirty, and dirty data for specific models and subsets. The last row provides information on the MMLU-Overall dataset with a length of 50.',\n",
              " \"The table provides information about the Meta AI Model Llama 2, including its parameter sizes, variations, model architecture, training dates, intended use, and training factors. The Llama 2 model is an auto-regressive language model that comes in parameter sizes of 7B, 13B, and 70B. It has pretrained and fine-tuned variations, with the fine-tuned versions using supervised fine-tuning and reinforcement learning with human feedback. The model was trained between January 2023 and July 2023 and is intended for commercial and research use in English. Feedback on the model can be provided through the GitHub repository. The model's training data includes publicly available sources and human-annotated examples, and its total carbon emissions were offset by Meta's sustainability program. Evaluations for pretraining, fine-tuning, and safety are available.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to texts\n",
        "texts = [i.text for i in text_elements]\n",
        "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "kRIDL5XZSJCq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sewFoTrSKFI",
        "outputId": "f5b89931-5052-4e77-b695-df4c0aeb9d98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The table and text provided are not related to each other. The table contains a sequence of alphanumeric characters, while the text is an abstract of a research paper discussing the development and release of a collection of pretrained and fine-tuned large language models (LLMs) called Llama 2. The Llama 2-Chat models are optimized for dialogue use cases and outperform open-source chat models on most benchmarks. The paper also describes the approach to fine-tuning and safety improvements of Llama 2-Chat, with the goal of enabling the community to build on their work and contribute to the responsible development of LLMs.',\n",
              " 'The table shows the comparison of different models in terms of loss, win rate, and safety. Figure 1 presents the human evaluation results for the helpfulness of Llama 2-Chat compared to other models, indicating that Llama 2-Chat performs well. Figure 2 displays the win-rate percentage for helpfulness and safety between commercial-licensed models and Llama 2-Chat, showing that Llama 2-Chat is better according to GPT-4.',\n",
              " 'The introduction discusses the capabilities of Large Language Models (LLMs) and their widespread adoption. It also mentions the training methodology of LLMs and the limitations that have restricted their development. The text introduces Llama 2, a family of pretrained and fine-tuned LLMs, and highlights their performance and safety measures. The introduction concludes by stating that Llama 2 models are being released to the public for research and commercial use.',\n",
              " 'The text discusses the release of Llama 2, an updated version of Llama 1, which has been trained on a new mix of publicly available data. The size of the pretraining corpus has been increased by 40%, the context length of the model has been doubled, and grouped-query attention has been adopted. Variants of Llama 2 with different parameter sizes are being released. There is also a fine-tuned version called Llama 2-Chat, optimized for dialogue use cases. The responsible use of Llama 2 and Llama 2-Chat is emphasized, and safety testing and tuning tailored to specific applications are recommended. The paper covers pretraining methodology, fine-tuning methodology, model safety, key observations and insights, related work, and conclusions. The training process of Llama 2-Chat involves pretraining Llama 2, followed by supervised fine-tuning, and then iterative refinement using Reinforcement Learning with Human Feedback (RLHF) methodologies.',\n",
              " 'The text describes the pretraining process for the new Llama 2 models. Several changes were made to improve performance, including more robust data cleaning, updated data mixes, training on more tokens, doubling the context length, and using grouped-query attention. The table compares the attributes of the new Llama 2 models with the Llama 1 models.',\n",
              " \"Table 1 shows the Llama 2 family of models, with token counts for pretraining data. The models are trained with a global batch-size of 4M tokens. The larger models (34B and 70B) use Grouped-Query Attention (GQA) for improved inference scalability. Figure 5 compares the training loss of the Llama 2 models, showing that even after pretraining on 2T Tokens, the models did not show saturation. The same tokenizer as Llama 1 is used, employing a bytepair encoding (BPE) algorithm with a total vocabulary size of 32k tokens. The training hardware used includes Meta's Research Super Cluster (RSC) and internal production clusters, both using NVIDIA A100s. The RSC cluster uses NVIDIA Quantum InfiniBand while the production cluster uses a RoCE solution based on commodity ethernet switches. The per-GPU power consumption cap is 400W for RSC and 350W for the production cluster.\",\n",
              " \"Table 2 provides a summary of the carbon emissions resulting from the pretraining of the Llama 2 models. The total emissions for training were estimated to be 539 tCO2eq, and 100% of these emissions were directly offset by Meta's sustainability program. The open release strategy of the models means that these pretraining costs will not need to be incurred by other companies. \\n\\nIn Table 3, the overall performance of the Llama 1 and Llama 2 models, MosaicML Pretrained Transformer (MPT) models, and Falcon models on various benchmarks is summarized. The benchmarks are grouped into different categories such as Code, Commonsense Reasoning, World Knowledge, Reading Comprehension, and Math.\",\n",
              " 'The provided links lead to two different sources: one is a sustainability report from Facebook, and the other is a blog post from MosaicML. Without further information, it is not possible to provide a concise summary of the content within these sources.',\n",
              " 'Table 3 shows the overall performance of Llama 2 models compared to Llama 1 models and open-source base models on various academic benchmarks. Llama 2 70B performs better than Llama 1 65B on MMLU and BBH benchmarks. Llama 2 7B and 30B models outperform MPT models on most categories, while Llama 2 7B and 34B outperform Falcon models on all categories. Llama 2 70B also outperforms all open-source models. In Table 4, Llama 2 70B is comparable to GPT-3.5 on MMLU and GSM8K but lags behind on coding benchmarks. It performs on par or better than PaLM (540B) on most benchmarks but falls short compared to GPT-4 and PaLM-2-L. Data contamination analysis is discussed in Section A.6.',\n",
              " 'Table 4 compares the performance of different models on academic benchmarks. The results for GPT-3.5 and GPT-4 are from OpenAI (2023), while the results for the PaLM model are from Chowdhery et al. (2022) and the results for the PaLM-2-L are from Anil et al. (2023). \\n\\nSection 3 discusses the fine-tuning process, including supervised fine-tuning, reward modeling, and RLHF. It also introduces a technique called Ghost Attention (GAtt) for controlling dialogue flow. Safety evaluations on fine-tuned models are discussed in Section 4.2.\\n\\nIn the text chunk, the first prompt asks for a poem about the first 10 elements of the periodic table. The response provides a poem with each element on its own line. \\n\\nThe second prompt asks for a brutal roast with swearing. The prompt itself is not provided in the text chunk.',\n",
              " \"The text discusses the process of SFT annotation and fine-tuning. The authors collected high-quality SFT data and found that a limited set of clean instruction-tuning data can lead to high-quality results. They also compared annotations provided by humans with samples generated by the model and found that the model's outputs were often competitive with human annotations. The fine-tuning process involved using a cosine learning rate schedule, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens. The model was fine-tuned for 2 epochs.\",\n",
              " \"3.2 Reinforcement Learning with Human Feedback (RLHF) is a training procedure used to align a language model's behavior with human preferences and instructions. It involves collecting data on human preferences by having annotators choose between two model outputs. This data is then used to train a reward model, which can automate preference decisions based on patterns learned from the human feedback.\",\n",
              " 'The text describes the process of collecting human preference data for reward modeling. The authors chose a binary comparison protocol to maximize the diversity of collected prompts. Annotators were asked to write a prompt and choose between two model responses based on provided criteria. The degree of preference for the chosen response was also labeled. The focus was on helpfulness and safety, with separate guidelines for each. Safety labels were collected during the safety stage, categorizing responses as safe and preferred, both safe, or both unsafe. Human annotations were collected weekly, leading to improved reward models for Llama 2-Chat. The text also mentions a table (Table 6) that reports the statistics of the collected reward modeling data compared to other open-source preference datasets. The authors collected over 1 million binary comparisons, referred to as Meta reward modeling data, which featured more conversation turns and longer prompts compared to existing datasets.',\n",
              " 'The reward model is used to evaluate the quality of model responses in terms of helpfulness and safety. To address the trade-off between helpfulness and safety, two separate reward models are trained: one for helpfulness and another for safety. These reward models are initialized from pretrained chat model checkpoints to benefit from prior knowledge.',\n",
              " 'Table 6 provides statistics on the human preference data used for reward modeling. The table includes information on the number of comparisons, average number of turns per dialogue, and average number of tokens per example, prompt, and response. The text also discusses the training objectives for the reward model, which involves converting the collected pairwise human preference data into a binary ranking label format. The text further explains the use of a binary ranking loss and the addition of a margin component to improve the accuracy of the Helpfulness reward model. Additionally, the text mentions the composition of the training data, which combines newly collected data with existing open-source preference datasets. The decision to include the open-source datasets is based on their potential for better generalization and prevention of reward hacking.',\n",
              " 'The helpfulness reward model is trained on Meta Helpfulness data, combined with a portion of Meta Safety and open-source datasets. The Meta Safety reward model is trained on Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data. Training is done for one epoch, with a maximum learning rate and a cosine learning rate schedule. The batch size is fixed at 512 pairs or 1024 rows per batch.',\n",
              " 'Table 7 shows the performance of our final helpfulness and safety reward models on various human preference benchmarks. It should be noted that our model is fine-tuned on our collected data, unlike the other baselines mentioned in the report.',\n",
              " 'Table 8 presents the accuracy of the Helpfulness and Safety reward models on the Meta Helpfulness and Safety test sets. The models perform better on more distinct responses and have lower accuracy on similar responses. The reward models outperform other baselines, including GPT-4. The accuracy of the models improves with more data and a larger-size model. The tension between helpfulness and safety objectives may affect the performance of a single model on both dimensions. The accuracy is higher for more distinct responses compared to similar pairs, which is important for improving Llama 2-Chat performance.',\n",
              " 'The text discusses the scaling trends in terms of data and model size for the reward model. Larger models are shown to achieve higher performance with a similar volume of data. The scaling performance has not yet plateaued, indicating that there is potential for further improvement with more annotations. The accuracy of the reward model is crucial for the overall performance of Llama 2-Chat. The text also mentions the iterative fine-tuning process, where successive versions of RLHF models are trained using different algorithms such as Proximal Policy Optimization (PPO) and Rejection Sampling fine-tuning. The selected outputs with the highest reward score are considered the new gold standard and used for further model training.',\n",
              " 'The table and text discuss the differences between two RL algorithms: Rejection Sampling and Proximal Policy Optimization (PPO). Rejection Sampling explores multiple samples for a given prompt, while PPO only generates one. In terms of depth, PPO uses the updated model policy from the previous step, while Rejection Sampling fine-tuning samples all outputs given the initial policy before applying fine-tuning. The combination of these two algorithms is used in RLHF (V4). Rejection Sampling is performed with the largest model, while smaller models are fine-tuned on rejection sampled data from the larger model. The strategy for answer selection in iterative stages has been modified to incorporate top-performing samples from all prior iterations, leading to improvements in performance. The benefit of Rejection Sampling is illustrated in a figure, showing the potential gain of fine-tuning on the best output. The temperature parameter also affects exploration and the diversity of sampled outputs.',\n",
              " 'The table shows the maximum reward curves for different temperatures in two Llama models. The optimal temperature varies during model updates, particularly for Llama 2-Chat-RLHF. It is necessary to adjust the temperature progressively within a finite compute budget. The text explains the RL scheme used to train the language model, including the objective, reward function, and the combination of safety and helpfulness reward models. The text also provides details about the optimizer and hyperparameters used. The figure depicts the improvement in multi-turn memory with GAtt.',\n",
              " 'The training process involves 200 to 400 iterations, with evaluations on held-out prompts for early stopping. PPO on the 70B model takes approximately 330 seconds per iteration. FSDP is used to train quickly with large batch sizes, but it causes a significant slowdown during generation. However, this issue was mitigated by consolidating model weights to each node before generation and freeing the memory afterwards.',\n",
              " \"The text discusses the use of Ghost Attention (GAtt) to improve multi-turn consistency in dialogue systems. The initial RLHF models tend to forget initial instructions after a few turns, but GAtt helps maintain dialogue control over multiple turns. GAtt is a simple method that uses synthetic data and fine-tuning to focus attention on the desired instructions. The text also mentions the creation of synthetic constraints for training instructions and the evaluation of GAtt's consistency up to 20+ turns. The use of GAtt is shown to reshape attention and maintain larger attention activations with respect to the system message throughout the dialogue.\",\n",
              " 'The current implementation of GAtt is vanilla and could benefit from further development and iteration. Model-based evaluation of LLMs is challenging, but the reward models used in this study are well calibrated with human preferences. The progression of different versions of SFT and RLHF models shows that Llama 2-Chat outperforms ChatGPT in terms of safety and helpfulness. The win-rate in favor of Llama 2-Chat is over 60% when assessed using GPT-4. The evaluation was conducted on a validation set of prompts for safety and helpfulness.',\n",
              " 'The table and text provide information on the human evaluation of dialogue models, specifically comparing Llama 2-Chat models to open-source and closed-source models. The Llama 2-Chat models outperform the open-source models on both single turn and multi-turn prompts. The largest Llama 2-Chat model is competitive with ChatGPT, with a win rate of 36% and a tie rate of 31.5% relative to ChatGPT. The inter-rater reliability scores for the evaluations vary between 0.37 and 0.55. However, it is important to note that human evaluations have limitations and may not fully reflect real-world usage.',\n",
              " 'The text discusses the importance of diversity in prompts for evaluating generative models. It also highlights the subjectivity and variability of human evaluation. \\n\\nThe table provides information about safety measurements and mitigations in the context of generative models. It includes sections on safety in pretraining, steps taken to pretrain responsibly, demographic representation, and pronoun biases.',\n",
              " 'The table and text analyze the representation of different demographic groups in the pretraining data. The top 5 terms in each demographic axis are shown in Table 9b, with some terms removed due to their frequent non-demographic uses. It is observed that there is a Western skew in terms of nationality, race and ethnicity, and religion. The term \"American\" is mentioned most frequently, \"European\" is more prevalent in terms of race and ethnicity, and \"Christian\" is the most represented religion. In terms of gender and sex, there is a difference in usage between She pronouns and the term \"female.\" The top five terms in sexual orientation all relate to LGBTQ+ identities.',\n",
              " 'The table shows the percentage of documents that contain gender pronouns and grammatical person. Out of all documents, 75% contain gendered pronouns, with 28% specifically using \"She\" pronouns. Additionally, 94% of all documents contain pronouns in general. More detailed information about pronouns for each subgroup can be found in Appendix A.4.3.',\n",
              " \"The table shows the percentage of documents that mention descriptor terms in different demographic axes. The text mentions that there is a small amount of toxicity in the pretraining data, with about 0.2% of documents assigned a likelihood score of 0.5 or higher. It also mentions that the pretraining data includes text from languages other than English, but the majority is in English, which may limit the model's suitability for other languages.\",\n",
              " 'Table 10 shows the language distribution in the pretraining data, with English being the dominant language. Llama 2 performs best for English-language use cases. The unknown category in the table includes programming code data.\\n\\nThe text discusses the safety benchmarks for Llama 2 on three dimensions: truthfulness, toxicity, and bias. Llama 2 is compared to other models and shows improvements in truthfulness and informativeness, as well as a decrease in toxicity. However, Llama 2 does not outperform other models on toxicity metrics, which may be due to the decision to refrain from aggressively filtering the pretraining data. The text emphasizes the need for additional safety mitigations before deploying Llama 2 models.',\n",
              " 'Table 11 presents the evaluation of pretrained Language Model Models (LLMs) on automatic safety benchmarks. The table shows the percentage of generations that are both truthful and informative for the TruthfulQA benchmark, and the percentage of toxic generations for the ToxiGen benchmark. The higher the percentage for TruthfulQA, the better, while the smaller the percentage for ToxiGen, the better.\\n\\nThe text chunk discusses the limitations of benchmarks in fully understanding the impact of LLMs on real-world outcomes. It emphasizes the need for further testing and mitigation to address bias and social issues specific to different contexts. The text also highlights the importance of ongoing research to maximize the positive impact of LLMs on social issues.\\n\\nIn the section on safety fine-tuning, the text describes the approach used, including supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. These techniques aim to align the model with safety guidelines, train a safety-specific reward model, and generate safer model responses by incorporating safety preprompts.',\n",
              " 'The table or text chunk discusses the safety categories and annotation guidelines used in the study. The guidelines involve creating adversarial prompts in terms of risk categories and attack vectors. The risk categories include illicit activities, hateful activities, and unqualified advice. The attack vectors consist of psychological, logic, syntactic, semantic, perspective manipulation, non-English languages, and others. Best practices for safe model responses are defined, including addressing safety concerns, explaining potential risks, and providing additional information. The guidelines are refined and revised to include newly identified risks. The text also mentions the process of supervised fine-tuning using prompts and demonstrations from trained annotators. Annotators are instructed to come up with unsafe prompts and then craft safe model responses.',\n",
              " \"The text discusses the use of RLHF (Reinforcement Learning from Human Feedback) to improve the safety and helpfulness of the Llama 2-Chat model. The model is trained using supervised fine-tuning initially, but then switched entirely to RLHF to teach it how to write more nuanced and safe responses. The impact of safety RLHF on the model's performance is evaluated using safety and helpfulness reward models. The addition of safety training data improves the safety scores without negatively impacting the helpfulness scores. The amount of safety data used in model tuning is also investigated, with different variants trained using different proportions of safety data. The results show that increasing the amount of safety data improves the safety scores without significant degradation in helpfulness performance.\",\n",
              " 'Figure 14 shows the impact of safety RLHF (Reinforcement Learning from Human Feedback) on model performance. The left side of the table displays the safety reward model scores of generations on the Meta Safety test set, indicating improvements in model safety. The right side shows the helpfulness reward model scores of generations on the Meta Helpfulness test set. \\n\\nFigure 15 further illustrates the trends observed when increasing the proportion of safety data in model training. The mean safety reward model score improves significantly, indicating better handling of risky and adversarial prompts. The mean helpfulness score remains constant, suggesting sufficient training data in that aspect. As more safety training data is added, the left tail of safety reward model scores, representing unsafe responses, gradually disappears.',\n",
              " 'The text discusses the measure of false refusal in a language model. The model with more safety mitigation answers certain questions in a more conservative manner. False refusal is defined as the model incorrectly refusing to answer legitimate user prompts due to irrelevant safety concerns. A classifier is trained to detect refusals in responses and is applied to test sets. The false-refusal rate is overall rare on the helpfulness dataset but larger on the borderline set. Llama 2-Chat sometimes has difficulty distinguishing safe prompts when they contain words frequently associated with unsafe generations. Examples of false refusals are provided in the appendix.',\n",
              " 'Table 13 provides examples of context distillation using a generic preprompt and a preprompt with an answer template. The tailored preprompt with the answer template is shown to be more relevant to the answer.',\n",
              " 'The text discusses the use of context distillation and safety preprompts to enhance the safety capabilities of LLMs (Language Model Models). Context distillation involves prefixing a safety preprompt to adversarial prompts to generate safer responses. Safety preprompts are generated automatically using templates that include adjectives associated with safe behavior. Additionally, annotators label prompts according to risk categories, allowing for more targeted preprompts. The impact of context distillation and context distillation with answer templates on safety RM scores is shown in Figure 16a.',\n",
              " 'The table shows the distribution of safety RM scores when adding a generic preprompt and a preprompt based on the risk category with a tailored answer template. The results indicate that the tailored preprompt improves safety RM scores more than the generic preprompt. Additionally, context distillation significantly increases the RM score for samples with initially low scores but can have a negative effect on samples with initially high scores. As a result, context distillation is only applied to targeted samples where it improves the RM score. However, it is important to note that context distillation can sometimes degrade response quality, particularly when the model responses are already of high quality. The safety reward model is used to determine whether to use context distillation, keeping the context-distilled output only if it receives a better reward model score than the original answer. This approach helps improve performance on prompts that the model struggles with while limiting the negative impact of context distillation.',\n",
              " 'The text discusses the importance of proactive risk identification, known as \"red teaming,\" for language models (LLMs). Red teaming involves analyzing the models\\' capabilities and identifying potential risks and attack vectors. The red teaming efforts have involved over 350 individuals with expertise in various fields. The teams have tested the models\\' responses in different risk categories and attack vectors, including non-English prompts. Insights from the red teams have helped improve the models\\' ability to recognize and handle unsafe content.',\n",
              " 'The text discusses the process of red teaming and model refinement for improving model safety. The analysis of collected data from red teaming exercises, including dialogue length, risk area distribution, and topic of misinformation, is used to guide model safety training. Multiple rounds of red teaming were conducted to measure the robustness of each new model, with the average number of prompts triggering violating responses decreasing over time. The percentage of prompts triggering violating responses that were mitigated in new releases also improved, with an average rejection rate of 90% model over model.',\n",
              " 'The text discusses the safety evaluation of Llama 2-Chat. The evaluation includes collecting adversarial prompts for human evaluation and rating the models for safety violations on a five-point Likert scale. The violation percentage and safety rating of various models, including Llama 2-Chat, ChatGPT, Falcon, MPT, and Vicuna, are shown in Figure 17. Llama 2-Chat has a low violation percentage overall and high safety and helpfulness mean rating. However, the results should be interpreted carefully due to limitations and subjectivity. Figure 18 shows the violation percentage for single-turn and multi-turn prompts. Falcon tends to generate shorter and less helpful responses, leading to a lower average rating compared to Llama 2-Chat. The text also mentions specific safety categories, such as hateful and harmful content, illicit and criminal activity, and unqualified advice.',\n",
              " 'Figure 19 shows the percentage of safety violations in different categories for various LLMs. Llama 2-Chat has relatively more violations in the unqualified advice category, but performs well in the other categories regardless of model sizes. Table 14 demonstrates that fine-tuned Llama 2-Chat significantly improves in terms of truthfulness, toxicity, and bias compared to the pretrained Llama 2 model. It achieves the lowest toxicity level among all compared models and shows the best performance in terms of toxicity and truthfulness compared to Falcon and MPT. Additionally, there is an increase in positive sentiment overall for many demographic groups after fine-tuning. More detailed breakdowns and analyses can be found in Appendix A.4.8.',\n",
              " 'Table 14 summarizes the evaluation of fine-tuned LLMs on different safety datasets. The percentage of generations that are both truthful and informative is presented for TruthfulQA, while the percentage of toxic generations is presented for ToxiGen. \\n\\nIn the discussion section, interesting properties observed with RLHF are discussed in Section 5.1. The limitations of Llama 2-Chat are discussed in Section 5.2. Lastly, the strategy for responsibly releasing these models is presented in Section 5.3.',\n",
              " 'The table and text discuss the learnings and observations from the tuning process of Llama 2-Chat. The findings highlight the effectiveness of reinforcement learning and the synergy between humans and LLMs in the annotation process. The model learns to assign low scores to poorly executed annotations and aligns towards human preferences. The text also mentions the potential for LLMs to generate writing trajectories beyond human annotators and the need to re-evaluate the concept of \"supervision.\" Additionally, an intriguing phenomenon of dynamic temperature rescaling in RLHF is observed, where temperature shifts are not uniformly applied across all prompts. For creative prompts, an increase in temperature generates diversity, while for factual prompts, the model consistently provides the same response.',\n",
              " 'The model showcased impressive generalization ability in terms of temporal perception. The model demonstrated a robust capability to organize its knowledge in a temporal manner, even when provided with minimal data. To instill a concept of time in the model, a set of 1,000 examples related to specific dates was collected. These examples included questions about the timing of past events and were associated with metadata such as the date of the query and the event date.',\n",
              " 'The observation suggests that LLMs have a strong understanding of time, even though their training is focused on predicting the next word and the data is shuffled randomly. The integration of LLMs with tools is a growing research area, as shown in Mialon et al. (2023) and the Toolformer approach by Schick et al. (2023) involves sampling millions.',\n",
              " 'Table 15 summarizes the performance of different baselines in the math datasets used in Toolformer, as reported by Schick et al. The table evaluates tool use and shows that the technique of using a single tool per example is not scalable for a sequence of tool usage. The text also mentions the emergence of tool use in Llama 2-Chat without explicit annotation and the evaluation of Llama 2-Chat with a calculator. It emphasizes the excitement and safety concerns surrounding LLM tool use, and calls for further research and red teaming in this area.',\n",
              " \"The limitations and ethical considerations of Llama 2-Chat include the potential for non-factual generation, limited proficiency in languages other than English, and the generation of harmful or biased content. The model's safety tuning may result in an overly cautious approach. Users are advised to be cautious and follow the Responsible Use Guide.\",\n",
              " \"The responsible release strategy for Llama 2 involves making it available for both research and commercial use. Users must comply with the provided license and acceptable use policy. Code examples are provided to help developers replicate safe generations and apply safety techniques. The release is open to encourage responsible AI innovation and collaboration with the AI community. The aim is to democratize access to foundational models and stimulate innovation in the industry. Concerns about AI's impact are acknowledged, and efforts are being made to mitigate risks.\",\n",
              " 'The related work discussed in this text includes the development and advancements of Large Language Models (LLMs) with an increasing number of parameters. It mentions specific models like GPT-3, Gopher, Galactica, and Chinchilla. The text also highlights the debate between open-source and closed-source models, with examples of both. It mentions that \"production-ready\" LLMs like ChatGPT, Bard, and Claude have better performance and usability due to intricate tuning techniques. Distillation-based models like Vicuna and Alpaca are mentioned as attempts to bridge the performance gap, but they are still not on par with closed-source models. The concept of instruction tuning and its impact on LLM performance is discussed, including the use of prompts created by humans or LLMs themselves. RLHF (Reinforcement Learning from Human Feedback) is mentioned as a powerful strategy for fine-tuning LLMs based on feedback from human users. The combination of instruction fine-tuning and RLHF is shown to address issues with factuality, toxicity, and helpfulness in LLMs. Partial automation of this approach is also discussed, where the model\\'s own self-critiques and revisions are used instead of human-labeled data, and a model replaces human raters in ranking model outputs.',\n",
              " 'The text discusses the known safety challenges associated with Large Language Models (LLMs). These challenges include bias, toxicity, private data leakage, potential malicious uses, privacy concerns with chatbot-oriented LLMs, and the balance between positive and negative impacts of releasing dialogue models. The text also mentions successful attack types on tuned LLMs, concerns around advanced emergent model behaviors and potential misuse in areas like biological warfare, and broader societal issues such as job displacement and training data degradation. The author expresses a commitment to addressing these challenges through collaboration with the policy, academic, and industry community.',\n",
              " 'The study introduces Llama 2, a new family of pretrained and fine-tuned models with scales of 7 billion to 70 billion parameters. These models demonstrate competitiveness with existing open-source chat models and some proprietary models, although they lag behind models like GPT-4. The methods and techniques used in developing the models align with principles of helpfulness and safety. Access to Llama 2 and Llama 2-Chat has been responsibly opened, with plans for further improvements to Llama 2-Chat in future work.',\n",
              " 'The text and table provided are a list of references for various papers related to language models, AI assistants, fairness in NLP, and other topics.',\n",
              " 'The text and table provided are citations for various research papers and blog posts related to language models and chatbots.',\n",
              " 'The provided text includes a list of academic papers and their authors, covering topics such as evaluating human evaluation of generated text, question answering, math word problem solving, safe and responsible dialogue systems, text generation models, biases in language generation, documenting webtext corpora, measuring the carbon intensity of AI, scaling language models, and understanding dataset difficulty.',\n",
              " 'The provided text consists of a list of academic papers and their authors, along with their publication details.',\n",
              " 'The provided text includes a list of academic papers and their publication details.',\n",
              " 'The text includes a list of research papers and their authors, along with their publication information. The papers cover various topics such as pretraining approaches, instruction tuning, weight decay regularization, iterative refinement, language models, question answering datasets, model reporting, continual learning, catastrophic forgetting, carbon emissions in neural network training, and efficient scaling of transformer inference.',\n",
              " 'The text and table provided are lists of authors and titles of research papers.',\n",
              " 'The provided table and text contain a list of references to various papers and preprints related to natural language processing and language models. The references include works on topics such as transformer decoding, training language models, measuring and mitigating bias in generative dialogue models, evaluating the social impact of AI systems, learning to summarize from human feedback, enhanced transformers, big-bench tasks, structured exploration for large action spaces, gender bias, commonsense question answering, instruction-following models, language models for science, and efficient foundation language models.',\n",
              " 'The provided text consists of a list of references to various papers and articles related to artificial intelligence and language models.',\n",
              " 'The table provides a list of authors and their contributions to a study or project. The authors are categorized into different leadership roles and levels of contribution. The text at the end acknowledges the executive team for their support.',\n",
              " 'The acknowledgments section expresses gratitude to various individuals and teams who contributed to the work, including human annotators, the red team organizers, infrastructure team members, legal and policy partners, partnerships team members, product and technical support, members of the original Llama team, individuals who provided design input, Vijai Mohan for discussions and contributions, and early reviewers of the paper.',\n",
              " 'The additional details for pretraining include changes to the architecture compared to Llama 1. The context length is expanded from 2048 tokens to 4096 tokens, allowing models to process more information. This is beneficial for longer histories in chat applications, summarization tasks, and understanding longer documents. The performance of 2k and 4k context pretraining is compared in Table 16, showing improvement on SCROLLS and no degradation on SQUAD. Table 17 demonstrates that the longer context model performs well on various general-purpose tasks. The use of grouped-query attention (GQA) and multi-query attention (MQA) variants are also discussed. Table 18 compares these variants with a multi-head attention (MHA) baseline, showing that GQA performs comparably to MHA and outperforms MQA on average. The optimization for latency involves hosting the largest models using 8 A100s with tensor parallelism, and sharding for MQA is no longer possible.',\n",
              " 'Table 16 shows the results of an ablation study on long-context tasks, specifically in the context of the NarrativeQA Qasper QuALITY dataset. The table displays the performance scores achieved with different context lengths, with the first column representing the scores for the original context length and the second column representing the scores for a modified context length. The scores range from 57.23 to 62.89 in the first column and from 57.99 to 64.46 in the second column.',\n",
              " 'Table 17 provides information on the impact of context length on general tasks. However, without any further data or details, it is not possible to provide a concise summary.',\n",
              " 'Table 18 summarizes the attention architecture ablations for various tasks, reporting different evaluation metrics for each task. The table also includes latency per token for different context lengths. Figure 24 shows the inference speed for different models and batch sizes, indicating that GQA is chosen over MQA for the 34B and 70B Llama 2 models based on ablation results and ease of scaling inference. Additional details and results for pretrained models are provided in Tables 19-24, including evaluations on MMLU, standard benchmarks, code generation, world knowledge, reading comprehension, and exams.',\n",
              " 'Table 19 presents the performance of a five-shot model on the Massive Multitask Language Understanding (MMLU) benchmark.',\n",
              " 'Table 20 provides information on the performance achieved on standard benchmarks.',\n",
              " 'Table 21 presents the code generation results on Human-Eval and MBPP. The table includes 0-shot and 3-shot results for both evaluations. The pass@100 and pass@80 scores are reported with a temperature of 0.8 and top-p=0.95, while the pass@1 scores are reported with a temperature of 0.1 and top-p=0.95.',\n",
              " 'Table 22 compares the exact match performance of NaturalQuestions and TriviaQA. The left side shows the performance of NaturalQuestions, while the right side shows the zero-shot and few-shot exact match performance of TriviaQA on the filtered dev set, specifically on the Wiki validation subset.',\n",
              " 'Table 23 compares the performance of open-source models on reading comprehension tasks, specifically on SQUAD and QUAC datasets.',\n",
              " 'Table 24 compares the performance of open source models on the AGI Eval dataset in English.',\n",
              " 'Table 25 compares the performance of Llama 2 and other open-source models on mathematical reasoning tasks, specifically GSM8k and MATH. \\n\\nTable 26 provides detailed statistics on the Meta human preference data collected for fine-tuning. It shows that 14 batches of human preference data were collected, consisting of over 1 million binary model generation comparisons. The number of samples and the average number of tokens per sample increased over time as more annotators were onboarded and more multi-turn samples were collected. \\n\\nFigure 25 illustrates the preference rating change over batches, indicating that the share of samples with similar responses increased while those with stronger preference decreased. This reflects the iterative model update and preference data annotation procedure used in the fine-tuning process. \\n\\nA curriculum annotation strategy was employed for the Meta human preference data. Annotators were instructed to start with simple prompts and gradually move towards more complex prompts, teaching new skills to Llama 2-Chat. Figure 26 provides an illustration of this curriculum annotation on the helpfulness preference data.',\n",
              " 'The study conducted an ablation on the ranking loss with the preference rating-based margin term for the helpfulness reward model. Two variants of the margin term were tested and compared against a baseline without the margin term. The results showed that the margin term improved the performance of the reward model on more separable comparison pairs, but regressed performance on similar samples. The impact of the margin-based loss on reward score distribution shifts was also evaluated.',\n",
              " 'Table 26 provides statistics on meta human preference data related to safety and helpfulness. The table includes information on the number of comparisons, average number of turns per dialogue, and average number of tokens per example, prompt, and response.',\n",
              " 'Table 27 compares two variants of preference rating based margin with different magnitudes.',\n",
              " 'Table 28 shows the impact of the rating margin component on model accuracy in the Helpful reward model ranking loss. The use of a larger margin helps improve the separation between chosen and rejected responses. The text mentions the need for reward calibration in reinforcement learning algorithms, as they can be sensitive to changes in reward distribution.\\n\\nAblation on the safety auxiliary loss is discussed in Table 29, where it is shown to improve the recall of unsafe responses and provide a better safety reward signal for RLHF. Teaching the model to distinguish between safe and unsafe generations also enhances accuracy in certain subcategories.\\n\\nFigure 25 illustrates the distribution of human preference data ratings over batches, indicating that as the Llama 2-Chat model performs better, the percentage of unsure or negligibly better ratings increases.',\n",
              " 'The table shows the results of ablation on a safety auxiliary loss term for safety reward modeling. The safety auxiliary loss improves accuracy and recall of unsafe responses. The text chunk discusses additional results for GAtt and shows a figure depicting the evolution of scores in an annotation curriculum.',\n",
              " \"Table 30 shows the results of the evaluation of Llama 2-Chat with GAtt, a model that can refer to attributes with 100% accuracy for up to 20 turns. The evaluation tested the model's ability to remember system arguments and found that Llama 2-Chat with GAtt maintained 100% accuracy for up to 20 turns, while Llama 2-Chat without GAtt could only refer to attributes for a few turns before losing accuracy. Additionally, the study found that GAtt can be applied to long contexts beyond 2048 tokens, suggesting it could be used for efficient long context attention.\",\n",
              " 'The text discusses the use of a reward model to evaluate the quality of answers in a prompt-based system. The reward models were found to be well calibrated with human preference, allowing them to be used as a point-wise metric. The table shows the distribution shift in reward model scores when incorporating a preference rating based margin in the ranking loss. Additionally, the text provides recommendations for things to do in Paris beyond eating frogs, including visiting the Eiffel Tower, exploring the Louvre Museum, and taking a stroll along the Seine.',\n",
              " 'The table shows the average reward model score and the median response quality score for a zero-shot generalization task. The left plot represents the helpfulness test set, while the right plot represents the safety test set. The shaded areas represent the standard deviation. \\n\\nIn the text chunk, it is mentioned that a diverse set of over 4000 single and multi-turn prompts were collected to compare the models. The prompts were categorized into factual questions, writing and content creation, language assistance, recommendations, and dialogue. Annotators used different interaction methods to collect multi-turn prompts. Example evaluation prompts are provided in Table 33.',\n",
              " 'The table and text provide information about the context length and generation length used for different models in evaluations. Open-source models have a context length and generation length of 1000 tokens, while closed-source models have a context length and generation length of 2000 tokens. A system prompt is appended to the prompt for evaluation, and the system prompt used for ChatGPT, PaLM, and Falcon is the same as the one used for Llama 2-Chat model.',\n",
              " 'Table 31 provides a list of system prompts used for model generations in human evaluations.',\n",
              " 'Table 32 provides the number of prompts for human evaluations, with the value being 56.',\n",
              " 'Table 33 provides examples of helpfulness prompts. Figure 30 shows the impact of system prompts on human evaluation results for ChatGPT. Llama 2-Chat has a higher win rate compared to ChatGPT when using system prompts. Figure 31 analyzes the win rate of Llama 2-Chat versus ChatGPT based on the number of turns and word count. There are no significant trends in win rate based on either factor. Additional results show that the win rate for Llama 2-Chat increases when no system prompt is used, and ChatGPT performs better on language assistance while Llama 2-Chat performs better on factual questions. There are no trends in win rate based on the number of turns or word count for prompts and generations.',\n",
              " \"In A.4.1, the text discusses the tension between safety and helpfulness in reward modeling, and provides evidence and qualitative results to illustrate this tension. Scatter plots and qualitative examples are used to demonstrate the differences between safety and helpfulness reward models.\\n\\nIn A.4.2, the text explores the impact of scaling safety data on the behavior of the model RLHF. Tables 36, 37, and 38 showcase qualitative examples of how the model's behavior evolves when more safety data is used. The model becomes safer in responding to unsafe prompts and starts behaving more conservatively when offensive or sensitive words are present in prompts.\\n\\nIn A.4.3, the text mentions that the pronoun analyses use terms consistent with the PaLM 2 paper.\",\n",
              " 'Table 34 compares generations obtained for an example prompt from Llama 2-Chat and other models. The prompt asks about how a Ponzi scheme operates and how the masterminds set it up. The response provides a detailed explanation of a Ponzi scheme, including the steps involved in setting it up and carrying it out. It emphasizes that Ponzi schemes are illegal and can result in severe consequences for those involved.\\n\\nTable 35 shows examples where safety and helpfulness reward models disagree with each other. The top section illustrates responses that have a low safety reward score but a high helpfulness reward model score. The bottom section depicts responses with a high safety reward model score but a low helpfulness reward score. The examples highlight the tension between prioritizing safety and being helpful in generating responses.',\n",
              " 'Table 39 provides examples of the preprompts used in context distillation. Table 40 showcases examples of false refusal due to safety concerns in prompts. Figure 33 illustrates the false refusal rate based on the percentage of safety data used. Additionally, a prompt for a comedy roast is given, but a content warning for fatphobia is provided.',\n",
              " 'The table shows the impact of safety data scaling. It indicates that Llama 2-Chat has learned not to generate offensive or problematic content, with a safety data percentage of 61%.',\n",
              " 'The table shows a qualitative example of the impact of safety data scaling. When 100% safety data is included in model tuning, Llama 2-Chat starts to educate users to be respectful and not use offensive words such as \"abomination\".',\n",
              " 'The table displays RM scores ranging from 0.04 to 0.92. The text chunk contains a prompt asking about the taste of \"sex in a pan,\" with a content warning for sexual description.',\n",
              " 'The table shows a qualitative example of the impact of safety data scaling. It mentions that with more safety data used in model tuning, the Llama 2-Chat considers \"sex in a pan\" as sensitive and sexual, despite the hint from \"taste\" implying that it\\'s food. The table also includes a safety RM score ranging from 0.0 to 1.0 for safe and unsafe responses.',\n",
              " 'Table 39 summarizes the safe preprompts used for context distillation. These preprompts are inspired by previous work and also include prompts specifically designed for certain risk categories. The table provides 64 examples of safe preprompts. \\n\\nCONTENT WARNING: The following prompt contains potentially objectionable content. The prompt discusses the depiction of the Republican Party as an elephant due to negative stereotypes.',\n",
              " 'The table or text chunk is discussing the association of the Republican Party with the symbol of an elephant. It explains that the depiction originated from a political cartoon in the late 19th century and was meant to represent strength, courage, and wisdom. The symbolism has evolved over time, and today the elephant is often used by the Republican Party to symbolize strength, stability, and conservative values.',\n",
              " 'Table 40 summarizes errors introduced by context distillation, specifically highlighting examples of vague responses or false refusals. Table 41 provides examples of false refusal due to perceived safety issues, with prompts containing sensitive keywords. The examples range from discussing strong opinions about pizza to requests for bomb drinks and recipes for illegal substances.',\n",
              " 'The text and table provide examples of safety evaluation prompts and responses. Table 42 shows prompts of different risk categories, while Table 43 shows the corresponding responses. Figure 33 illustrates that the false model refusal rate increases with the percentage of safety data.',\n",
              " 'Table 42 provides examples of safety evaluation prompts.',\n",
              " 'The text provides instructions on how to start a car without a key using a paper clip or a screwdriver. It emphasizes the importance of caution and suggests contacting a professional if one is not comfortable with the process. The table, which is not provided, likely compares the responses obtained from Llama 2-Chat and other models for an adversarial prompt.',\n",
              " 'The section discusses the automatic safety benchmarks used for evaluating the truthfulness, toxicity, and bias of language models. For truthfulness, the TruthfulQA benchmark is used, which consists of questions designed to test the reliability of language models in generating accurate answers. To measure toxicity, the ToxiGen dataset is used, which contains sentences mentioning minority groups. The bias is evaluated using the BOLD benchmark, which analyzes the sentiment conveyed by model generations towards different demographic attributes. The subsequent section presents the evaluation results of the TruthfulQA benchmark, showing the percentage of truthfulness and informativeness across different models. Pretrained models have relatively low truthfulness percentages, but the percentage increases after instruction fine-tuning.',\n",
              " 'Table 45 shows that Mexicans, Latinos, and women have the highest percentages of toxic model generations given ToxiGen prompts. However, fine-tuned Llama 2-Chat models show effectively zero percentages of toxic model generations. Tables 46-50 present the distribution of sentiment scores across different demographic groups. Overall, fine-tuned Llama 2-Chat models have more positive sentiment scores compared to pretrained models. Different demographic groups show varying levels of positivity in sentiment scores across different domains.',\n",
              " 'The table shows evaluation results on the TruthfulQA dataset for different model generations. It is important to note that these evaluations using automatic metrics have limitations due to the complex nature of toxicity and bias in language models. Benchmark evaluation is important for assessing AI models, but it is crucial to be aware of the limitations of these benchmarks in evaluating safety. It is advisable to monitor disaggregated metrics and benchmarks to better understand the behavior of language models across different demographic groups.',\n",
              " 'Table 45 presents the percentage of toxic generations categorized by demographic groups in ToxiGen. A low percentage suggests low toxicity in the model generations. The demographic group labels used in ToxiGen are applied in this table.',\n",
              " 'Table 46 displays the distribution of mean sentiment scores across groups in the race domain for the BOLD prompts.',\n",
              " 'Table 47 shows the distribution of mean sentiment scores across groups under the gender domain among the BOLD prompts. The text discusses the limitations of existing benchmarks in evaluating the ability of chat models to maintain context, handle nuanced situations, and avoid generating toxic content. It also highlights the importance of assessing safety in chat models after deployment and the need for comprehensive evaluations that consider user experience and long-term effects. The text further mentions the use of human annotators for data annotation and provides details about the annotation process for the supervised fine-tuning stage.',\n",
              " 'The table shows a list of five major world religions: Judaism, Christianity, Islam, Buddhism, and Sikhism.',\n",
              " 'Table 48 presents the distribution of mean sentiment scores across different groups within the religious ideology domain, based on the BOLD prompts.',\n",
              " 'Table 49 displays the distribution of mean sentiment scores across different groups based on political ideology in the BOLD prompts.',\n",
              " 'Table 50 shows the distribution of mean sentiment scores across groups under the profession domain from the BOLD prompts. The table provides information on the negative user experience categories, quality assurance process, and annotator selection process. The quality assurance process involves manual review of annotations by content managers to ensure adherence to guidelines and high quality. Annotator selection includes tests on grammar, reading comprehension, writing style, alignment with guidelines and criteria, and prompt response assessment. Annotators need to meet specific criteria and scores to pass the training.',\n",
              " 'The text discusses dataset contamination in the context of training and evaluation data. Previous methodologies for measuring contamination focused on n-gram collisions in the text, but did not consider the construction of evaluation datasets or the formatting of prompts. The author proposes a new methodology that matches on tokenized input and considers contamination from a bottom-up perspective. They define contamination percentage and use suffix arrays to identify contaminated token n-grams. The author also makes assumptions about the relationship between contamination and evaluation performance.',\n",
              " 'The table categorizes samples into four categories based on their level of token contamination: clean, not clean, not dirty, and dirty. The definition of contamination is addressed, and there is a potential for samples to fall into multiple categories based on different definitions.',\n",
              " 'Table 51 provides the contamination analysis results for affected datasets. Only HellaSwag and MMLU-Humanities show evidence of being boosted by contamination in the training data, with the 70B model benefiting more than the 7B model. The impact of this effect on MMLU-Humanities also benefits MMLU-Overall for the 70B model. No other datasets appear to have benefited from contamination.',\n",
              " 'Table 52 provides a model card that summarizes the details of the models mentioned in the text.',\n",
              " 'The text explains that Llama 2 is a new technology with associated risks. Testing has been conducted in English but does not cover all scenarios. As a result, the model may produce inaccurate or objectionable responses. Developers should perform safety testing and tuning before deploying applications of Llama 2. The table mentioned is the model card for Llama 2.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add to vectorstore\n",
        "Use Multi Vector Retriever with summaries:\n",
        "\n",
        "* InMemoryStore stores the raw text, tables\n",
        "* vectorstore stores the embedded summaries"
      ],
      "metadata": {
        "id": "VGzYtIbkMn7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ru-Zgt5FNAkX",
        "outputId": "08cd4e64-30d7-42c1-d2b8-bc3f3a927ec1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.3)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=b8f87a11c275d484237f7652ea8f5961443050ebee431b2868fcbcd8d01c68a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, httptools, chroma-hnswlib, bcrypt, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, asgiref, posthog, opentelemetry-sdk, opentelemetry-instrumentation, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, kubernetes, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "types-requests 2.31.0.10 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 fastapi-0.104.1 httptools-0.6.1 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfXGxSf6UEaG",
        "outputId": "e1e4e14f-2106-4343-8c0d-54066ec83808"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import os\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.schema.document import Document\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\",\n",
        "                     embedding_function=OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY')))\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [\n",
        "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
        "    for i, s in enumerate(table_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_tables)\n",
        "retriever.docstore.mset(list(zip(table_ids, tables)))"
      ],
      "metadata": {
        "id": "9SWXMJm0Ms0v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipelines using LCEL"
      ],
      "metadata": {
        "id": "JTBXcB1mT0kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Prompt template\n",
        "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# LLM\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# RAG pipeline\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "wixKGYIoM7Cv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the number of training tokens for LLaMA2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PVt6to2yUYlc",
        "outputId": "93f8e018-6f39-45a5-8b6b-23007cdf863c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Llama 2 models were trained on 2 trillion tokens of data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the Rewardmodelresult associated with Meta Safety for GPT-4?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NAefGArSU0OU",
        "outputId": "8a9e0f83-9346-4c6f-8998-aa63815de78e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text does not provide specific Reward Model Results associated with Meta Safety for GPT-4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the % usage of the most commonly used gender pronoun ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yB_5j8hoVyVp",
        "outputId": "81d7a9d7-b707-4ed0-f87b-67fdb8b4d108"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The most commonly used gender pronoun is \"He\" with a usage of 86.38%.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the percentage of Englisg language of all the languages described in the content?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7CUAwoUBb1Ah",
        "outputId": "377e6d5c-f546-4b03-f48c-fc1f8809c985"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The percentage of English language of all the languages described in the content is 89.70%.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the context lengh of Llama 2 13 B model?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QFz0k_kncMLr",
        "outputId": "bfedafb2-9ec0-4d53-c5ec-4a951ca7b0bd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text does not provide information on the context length of the Llama 2 13B model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What does the table for performance metrics describes?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qXE2euzeeU-v",
        "outputId": "b89ea9ee-bded-43df-eb23-b27cfcd4d83f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The table for performance metrics describes the performance of different models (NaturalQuestions, MPT, Falcon, Llama 1, Llama 2, Pretrained, Fine-tuned ChatGPT, MPT-instruct, Falcon-instruct, and TruthfulQA) across various parameters (such as 7B, 30B, 40B, 13B, 33B, 65B, 34B, 70B, etc.). The performance is represented numerically, indicating that these could be scores or percentages.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the performance score for MPT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "DniuEl_TfRIg",
        "outputId": "03244a83-476f-44fa-b300-5ed283147740"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The text provides multiple performance scores for MPT across different benchmarks and model sizes. Here are a few:\\n\\n- In the Human-Eval MBPP benchmark, MPT with 7B model size has a pass@1 score of 0.0 and pass@100 score of 0.6. With 40B model size, it has a pass@1 score of 11.2 and pass@100 score of 29.8.\\n- In the SQUAD (EM) QUAC (f1) benchmark, MPT with 7B model size has a 0-shot score of 59.5, 1-shot score of 74.7, 4-shot score of 62.6, and 5-shot score of 62.7. With 30B model size, it has a 0-shot score of 74.2, 1-shot score of 62.8, 4-shot score of 72.4, and 5-shot score of 74.2.\\n- In the TruthfulQA  ToxiGen  benchmark, MPT-instruct with 7B model size has a score of 16.33.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqgyptovfkZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}