{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Langchain_usecases/blob/main/langchain_index_zapier_agent_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**! This Colab has a video tutorial. Find it here https://youtu.be/1towAoXOWLg**"
      ],
      "metadata": {
        "id": "LF9UY_Ou2Eot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "i3rmZlgg0RsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "khCtKh6eDzh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.157\n",
        "!pip install pypdf\n",
        "!pip install pinecone-client\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "mxFhFfKzRgpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up OpenAI API key"
      ],
      "metadata": {
        "id": "oU2eHKKCD4lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "hOiNazy2SSPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up Pinecone API keys"
      ],
      "metadata": {
        "id": "6xUZg07MMvoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone \n",
        "\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=\"YOUR_PINECONE_API_KEY\",  # find at app.pinecone.io\n",
        "    environment=\"YOUR_ENVIRONMENT_NAME\"  # next to api key in console\n",
        ")"
      ],
      "metadata": {
        "id": "7hai7fChMzw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index "
      ],
      "metadata": {
        "id": "vCnFhGb40ZxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data from PDF**\n",
        "\n",
        "Load PDF using pypdf into array of documents, where each document contains the page content and metadata with page number.\n",
        "\n",
        "For loading other formats, check the [documentation](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)"
      ],
      "metadata": {
        "id": "kNrHppQSEQU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcUaA9YWRS2D"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"PATH_TO_YOUR_FILE\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the text from pdf into smaller chunks**\n",
        "\n",
        "There are many ways to split the text. We are using the text splitter that is recommended for generic texts. For more ways to slit the text check the [documentation](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)"
      ],
      "metadata": {
        "id": "N0Q03G2HG-6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "DV9zorYiHAzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings"
      ],
      "metadata": {
        "id": "BtIKffZiFdpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "KK-LlnZJFPMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a vectorstore**\n",
        "\n",
        "A vectorstore stores Documents and associated embeddings, and provides fast ways to look up relevant Documents by embeddings. \n",
        "\n",
        "There are many ways to create a vectorstore. We are going to use Pinecone. For other types of vectorstores visit the [documentation](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)\n",
        "\n",
        "First you need to go to [Pinecone](https://www.pinecone.io/) and create an index there. Then type the index name in \"index_name\""
      ],
      "metadata": {
        "id": "5sY05f0sFl_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "index_name = \"index_name\"\n",
        "\n",
        "#create a new index\n",
        "docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
        "\n",
        "# if you already have an index, you can load it like this\n",
        "# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
      ],
      "metadata": {
        "id": "cng7EmkbLyBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you cannot create a Pinecone account, try to use CromaDB. The following code creates a transient in-memory vectorstore. For further information check the [documentation](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/chroma.html). \n",
        "\n",
        "The following code block uses Croma for creating a vectorstore. Uncomment it if you don't have access to pinecone and use it instead."
      ],
      "metadata": {
        "id": "L9YIfNAQP_M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import Chroma\n",
        "# docsearch = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "4fNRT9mRP-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorstore is ready. Let's try to query our docsearch with similarity search"
      ],
      "metadata": {
        "id": "DKpbxw5JRe2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is DesignOps support model?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "# print(len(docs))\n",
        "# print(docs[0])\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "xqqD72sGRbBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a question answering chain\n",
        "The question answering chain will enable us to generate the answer based on the relevant context chunks. See the [documentation](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) for more explanation."
      ],
      "metadata": {
        "id": "Q_4iKKJ16RCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain import OpenAI\n",
        "\n",
        "#defining LLM\n",
        "llm = OpenAI(temperature=0.2)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}))"
      ],
      "metadata": {
        "id": "UAVa_0Tk_ckr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the chain to test that it's working"
      ],
      "metadata": {
        "id": "Y1HYYCw4ErzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is DesignOps support model?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "9FcufORSOtJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Zapier tool\n",
        "First, you need to get a Zapier API key here https://nla.zapier.com/get-started/ and add the the actions that you are going to use in Zapier"
      ],
      "metadata": {
        "id": "8R9ofw65HSJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"ZAPIER_NLA_API_KEY\"] = os.environ.get(\"ZAPIER_NLA_API_KEY\", \"YOUR_ZAPIER_API_KEY\")"
      ],
      "metadata": {
        "id": "y9hBdBM7Hyuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a zapier toolkit. For more information visit [documentation](https://python.langchain.com/en/latest/modules/agents/tools/examples/zapier.html)"
      ],
      "metadata": {
        "id": "LYPMP5bFIB-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_toolkits import ZapierToolkit\n",
        "from langchain.utilities.zapier import ZapierNLAWrapper\n",
        "\n",
        "zapier = ZapierNLAWrapper()\n",
        "toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)"
      ],
      "metadata": {
        "id": "Am89AT20IMNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to see Zapier toolkit\n",
        "# tools = toolkit.get_tools()\n",
        "# print(tools[0])"
      ],
      "metadata": {
        "id": "zsKl90NCIogb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building agent\n",
        "Assembling it all together into an agent. For more information check the documentation https://python.langchain.com/en/latest/modules/agents/agent_executors/examples/agent_vectorstore.html and https://python.langchain.com/en/latest/modules/agents/tools/examples/zapier.html and https://python.langchain.com/en/latest/modules/agents/agents/examples/conversational_agent.html\n"
      ],
      "metadata": {
        "id": "ThDUffJAl4-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "#defining the tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Demo\",\n",
        "        func=qa.run,\n",
        "        description=\"use this as the primary source of context information when you are asked the question. Always search for the answers using this tool first, don't make up answers yourself\"\n",
        "    ),\n",
        "] + toolkit.get_tools()\n",
        "\n",
        "\n",
        "#setting a memory for conversations\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "#Setting up the agent \n",
        "agent_chain = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)"
      ],
      "metadata": {
        "id": "Izm_WhQUqY_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quering the agent\n",
        "\n",
        "*To get an agent do what you want, the prompt should be constructed properly. For user facing apps, we need to look at prompt templates and also figure out if chat is the best interface*"
      ],
      "metadata": {
        "id": "d0rc89O0jkuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"What Adrienne Allnutt have said about DesignOps?\")"
      ],
      "metadata": {
        "id": "7NGPCmdhm5dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Email the answer to email@gmail.com and mention that this email was sent by AI\")"
      ],
      "metadata": {
        "id": "MRjrL-hzn5Cv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}