{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPo7SWL919vExwBWegpDPVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b2816f55c6b4f11aa72bda926291f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e147a9ccb57409aa88c50fe44a9264a",
              "IPY_MODEL_199597bbc311433ab2fee4154d1322ea",
              "IPY_MODEL_115d1cd69ad04562a8b134e7359f9d43"
            ],
            "layout": "IPY_MODEL_0481e925a8b740d68908b9838adbc50a"
          }
        },
        "0e147a9ccb57409aa88c50fe44a9264a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fb34447e6164acc949df4d23077b524",
            "placeholder": "​",
            "style": "IPY_MODEL_86db8a52dcee4b84b4c274c181e17eaa",
            "value": "Fetching 5 files: 100%"
          }
        },
        "199597bbc311433ab2fee4154d1322ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e3ea6150f064d1b9dbda0267085f2ab",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fcc3d04b2fe4d74a6ebf97e53dd87c2",
            "value": 5
          }
        },
        "115d1cd69ad04562a8b134e7359f9d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3961d2e124c141ac95346dda98f866de",
            "placeholder": "​",
            "style": "IPY_MODEL_2e647df92ff442c4842b784a5661f647",
            "value": " 5/5 [00:00&lt;00:00, 244.79it/s]"
          }
        },
        "0481e925a8b740d68908b9838adbc50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fb34447e6164acc949df4d23077b524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86db8a52dcee4b84b4c274c181e17eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e3ea6150f064d1b9dbda0267085f2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcc3d04b2fe4d74a6ebf97e53dd87c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3961d2e124c141ac95346dda98f866de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e647df92ff442c4842b784a5661f647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Langchain_usecases/blob/main/Building_a_multidocument_agentic_RAG_with_LLamaIndex_GROQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY0OsrGT4ViX",
        "outputId": "764522b3-2f62-4976-80f0-96eebadc864c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "llama-index\n",
        "llama-index-llms-huggingface\n",
        "llama-index-embeddings-fastembed\n",
        "fastembed\n",
        "Unstructured[md]\n",
        "chromadb\n",
        "llama-index-vector-stores-chroma\n",
        "llama-index-llms-groq\n",
        "einops\n",
        "accelerate\n",
        "sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Dependencies"
      ],
      "metadata": {
        "id": "lNYRlgiO7TqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF1WOSgN71hC",
        "outputId": "3cc7c394-dedc-4530-8f8b-99afabf1f505"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index (from -r requirements.txt (line 1))\n",
            "  Downloading llama_index-0.10.36-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-llms-huggingface (from -r requirements.txt (line 2))\n",
            "  Downloading llama_index_llms_huggingface-0.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-embeddings-fastembed (from -r requirements.txt (line 3))\n",
            "  Downloading llama_index_embeddings_fastembed-0.1.4-py3-none-any.whl (2.7 kB)\n",
            "Collecting fastembed (from -r requirements.txt (line 4))\n",
            "  Downloading fastembed-0.2.7-py3-none-any.whl (27 kB)\n",
            "Collecting Unstructured[md] (from -r requirements.txt (line 5))\n",
            "  Downloading unstructured-0.13.7-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb (from -r requirements.txt (line 6))\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-vector-stores-chroma (from -r requirements.txt (line 7))\n",
            "  Downloading llama_index_vector_stores_chroma-0.1.8-py3-none-any.whl (4.8 kB)\n",
            "Collecting llama-index-llms-groq (from -r requirements.txt (line 8))\n",
            "  Downloading llama_index_llms_groq-0.1.3-py3-none-any.whl (2.7 kB)\n",
            "Collecting einops (from -r requirements.txt (line 9))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirements.txt (line 10))\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from -r requirements.txt (line 11))\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.35 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_core-0.10.36-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_llms_openai-0.1.18-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting huggingface-hub<0.24.0,>=0.23.0 (from llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (4.40.2)\n",
            "INFO: pip is looking at multiple versions of fastembed to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastembed (from -r requirements.txt (line 4))\n",
            "  Downloading fastembed-0.2.6-py3-none-any.whl (26 kB)\n",
            "  Downloading fastembed-0.2.5-py3-none-any.whl (26 kB)\n",
            "  Downloading fastembed-0.2.4-py3-none-any.whl (26 kB)\n",
            "  Downloading fastembed-0.2.3-py3-none-any.whl (26 kB)\n",
            "  Downloading fastembed-0.2.2-py3-none-any.whl (22 kB)\n",
            "Collecting llama-index-embeddings-fastembed (from -r requirements.txt (line 3))\n",
            "  Downloading llama_index_embeddings_fastembed-0.1.3-py3-none-any.whl (2.7 kB)\n",
            "INFO: pip is looking at multiple versions of fastembed to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting fastembed (from -r requirements.txt (line 4))\n",
            "  Downloading fastembed-0.2.1-py3-none-any.whl (22 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading fastembed-0.1.3-py3-none-any.whl (14 kB)\n",
            "  Downloading fastembed-0.1.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading fastembed-0.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting onnx<2.0,>=1.11 (from fastembed->-r requirements.txt (line 4))\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<2.0,>=1.15 (from fastembed->-r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 4)) (2.31.0)\n",
            "Collecting tokenizers<0.14,>=0.13 (from fastembed->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0,>=4.65 in /usr/local/lib/python3.10/dist-packages (from fastembed->-r requirements.txt (line 4)) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface->-r requirements.txt (line 2)) (4.11.0)\n",
            "Collecting llama-index-embeddings-fastembed (from -r requirements.txt (line 3))\n",
            "  Downloading llama_index_embeddings_fastembed-0.1.2-py3-none-any.whl (2.7 kB)\n",
            "  Downloading llama_index_embeddings_fastembed-0.1.1-py3-none-any.whl (2.9 kB)\n",
            "  Downloading llama_index_embeddings_fastembed-0.1.0-py3-none-any.whl (2.9 kB)\n",
            "INFO: pip is looking at multiple versions of llama-index-embeddings-fastembed to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading llama_index_embeddings_fastembed-0.0.1-py3-none-any.whl (2.7 kB)\n",
            "Collecting llama-index-llms-huggingface (from -r requirements.txt (line 2))\n",
            "  Downloading llama_index_llms_huggingface-0.1.5-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: huggingface-hub<0.21.0,>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading openai-1.28.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (8.3.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.14.1)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed->-r requirements.txt (line 4))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.16,>=0.15 (from fastembed->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from Unstructured[md]->-r requirements.txt (line 5)) (5.2.0)\n",
            "Collecting filetype (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from Unstructured[md]->-r requirements.txt (line 5)) (4.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from Unstructured[md]->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from Unstructured[md]->-r requirements.txt (line 5)) (4.12.3)\n",
            "Collecting emoji (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading rapidfuzz-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting unstructured-client (from Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from Unstructured[md]->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 6)) (1.2.1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 6)) (2.7.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 6)) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 6)) (1.63.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 6)) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai-like<0.2.0,>=0.1.3 (from llama-index-llms-groq->-r requirements.txt (line 8))\n",
            "  Downloading llama_index_llms_openai_like-0.1.3-py3-none-any.whl (3.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 10)) (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 11)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 11)) (1.11.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 6)) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6)) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (2.0.7)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->Unstructured[md]->-r requirements.txt (line 5)) (2.5)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (2023.12.25)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0,>=1.11->fastembed->-r requirements.txt (line 4)) (3.20.3)\n",
            "Collecting coloredlogs (from onnxruntime<2.0,>=1.15->fastembed->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0,>=1.15->fastembed->-r requirements.txt (line 4)) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0,>=1.15->fastembed->-r requirements.txt (line 4)) (1.12)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6)) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6)) (67.7.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 6)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 6)) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed->-r requirements.txt (line 4)) (3.7)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface->-r requirements.txt (line 2))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers->-r requirements.txt (line 11))\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 11)) (3.5.0)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (4.0.3)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->Unstructured[md]->-r requirements.txt (line 5))\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.9.0 (from chromadb->-r requirements.txt (line 6))\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb->-r requirements.txt (line 6))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (13.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 6)) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb->-r requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (3.0.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2.0,>=1.15->fastembed->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0,>=1.15->fastembed->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index->-r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb->-r requirements.txt (line 6)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 6)) (0.6.0)\n",
            "Building wheels for collected packages: pypika, langdetect\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=07d15da6eeef1393e8fb592ac6d3b9f9528a56a9a7ef69034ca5bfc9c98f0e90\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=bfd33b24105dd5ed943edd1dcff6da38ea100de61925c7428e55b4960db20f38\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built pypika langdetect\n",
            "Installing collected packages: striprtf, pypika, monotonic, mmh3, filetype, dirtyjson, websockets, uvloop, ujson, shellingham, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pypdf, overrides, orjson, ordered-set, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, loguru, langdetect, jsonpath-python, importlib-metadata, humanfriendly, httptools, h11, emoji, einops, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, deepdiff, coloredlogs, typer, tokenizers, text-generation, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, nvidia-cusolver-cu12, kubernetes, httpx, dataclasses-json, unstructured-client, transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, llamaindex-py-client, fastembed, Unstructured, sentence-transformers, opentelemetry-instrumentation-fastapi, llama-index-legacy, llama-index-core, accelerate, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-fastembed, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-openai-like, llama-index-llms-huggingface, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-llms-groq, llama-index-question-gen-openai, llama-index, fastapi-cli, fastapi, chromadb, llama-index-vector-stores-chroma\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Unstructured-0.13.7 accelerate-0.30.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 dataclasses-json-0.6.6 deepdiff-7.0.1 deprecated-1.2.14 dirtyjson-1.0.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.1.1 emoji-2.11.1 fastapi-0.111.0 fastapi-cli-0.0.3 fastembed-0.2.7 filetype-1.2.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.0.0 jsonpath-python-1.0.6 kubernetes-29.0.0 langdetect-1.0.9 llama-index-0.10.36 llama-index-agent-openai-0.2.4 llama-index-cli-0.1.12 llama-index-core-0.10.36 llama-index-embeddings-fastembed-0.1.4 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-groq-0.1.3 llama-index-llms-huggingface-0.1.5 llama-index-llms-openai-0.1.18 llama-index-llms-openai-like-0.1.3 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.4 llama-index-vector-stores-chroma-0.1.8 llama-parse-0.4.2 llamaindex-py-client-0.1.19 loguru-0.7.2 marshmallow-3.21.2 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 onnx-1.16.0 onnxruntime-1.17.3 openai-1.28.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 ordered-set-4.1.0 orjson-3.10.3 overrides-7.7.0 posthog-3.5.0 pypdf-4.2.0 pypika-0.48.9 python-dotenv-1.0.1 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 rapidfuzz-3.9.0 sentence-transformers-2.7.0 shellingham-1.5.4 starlette-0.37.2 striprtf-0.0.26 text-generation-0.7.0 tiktoken-0.6.0 tokenizers-0.15.2 transformers-4.39.3 typer-0.12.3 typing-inspect-0.9.0 ujson-5.9.0 unstructured-client-0.22.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-llms-openai"
      ],
      "metadata": {
        "id": "pJroyUAuuqMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Documents to be processed"
      ],
      "metadata": {
        "id": "dx4tpjTk8V3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "8aKgfwaT747z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget \"https://arxiv.org/pdf/1810.04805.pdf\" -O ./data/BERT_arxiv.pdf\n",
        "! wget \"https://arxiv.org/pdf/2005.11401\" -O ./data/RAG_arxiv.pdf\n",
        "! wget \"https://arxiv.org/pdf/2310.11511\" -O ./data/self_rag_arxiv.pdf\n",
        "! wget \"https://arxiv.org/pdf/2401.15884\" -O ./data/crag_arxiv.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFtmjS1z9CEQ",
        "outputId": "990dae82-275d-452c-bbf4-782cdbe9424b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-11 15:51:49--  https://arxiv.org/pdf/1810.04805.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/1810.04805 [following]\n",
            "--2024-05-11 15:51:49--  http://arxiv.org/pdf/1810.04805\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775166 (757K) [application/pdf]\n",
            "Saving to: ‘./data/BERT_arxiv.pdf’\n",
            "\n",
            "\r./data/BERT_arxiv.p   0%[                    ]       0  --.-KB/s               \r./data/BERT_arxiv.p 100%[===================>] 757.00K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-11 15:51:49 (43.8 MB/s) - ‘./data/BERT_arxiv.pdf’ saved [775166/775166]\n",
            "\n",
            "--2024-05-11 15:51:49--  https://arxiv.org/pdf/2005.11401\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 885323 (865K) [application/pdf]\n",
            "Saving to: ‘./data/RAG_arxiv.pdf’\n",
            "\n",
            "./data/RAG_arxiv.pd 100%[===================>] 864.57K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-11 15:51:49 (46.5 MB/s) - ‘./data/RAG_arxiv.pdf’ saved [885323/885323]\n",
            "\n",
            "--2024-05-11 15:51:49--  https://arxiv.org/pdf/2310.11511\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1405127 (1.3M) [application/pdf]\n",
            "Saving to: ‘./data/self_rag_arxiv.pdf’\n",
            "\n",
            "./data/self_rag_arx 100%[===================>]   1.34M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-11 15:51:49 (55.1 MB/s) - ‘./data/self_rag_arxiv.pdf’ saved [1405127/1405127]\n",
            "\n",
            "--2024-05-11 15:51:49--  https://arxiv.org/pdf/2401.15884\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 643848 (629K) [application/pdf]\n",
            "Saving to: ‘./data/crag_arxiv.pdf’\n",
            "\n",
            "./data/crag_arxiv.p 100%[===================>] 628.76K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-11 15:51:49 (37.8 MB/s) - ‘./data/crag_arxiv.pdf’ saved [643848/643848]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Dependencies"
      ],
      "metadata": {
        "id": "mgBUMGLN-KMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,SummaryIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool,QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters,FilterCondition\n",
        "from typing import List,Optional"
      ],
      "metadata": {
        "id": "z5S6NHjs-RuA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "LDhb9SMqBAqS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "id": "P7asZW-QBQdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files = ['./data/self_rag_arxiv.pdf']).load_data()"
      ],
      "metadata": {
        "id": "ZbncHhLnBEQ3"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XYwP3cwYUwIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGqtM0NABlDS",
        "outputId": "bdbf61ae-c0d8-4ecf-bd4d-877624220618"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Metadata"
      ],
      "metadata": {
        "id": "3RGuzGwgBxI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2GI96aBBmaQ",
        "outputId": "a89a456b-76e5-435a-e254-eeee10e5029f"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page_label': '1',\n",
              " 'file_name': 'self_rag_arxiv.pdf',\n",
              " 'file_path': 'data/self_rag_arxiv.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 1405127,\n",
              " 'creation_date': '2024-05-11',\n",
              " 'last_modified_date': '2023-10-19'}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the documents into chunks"
      ],
      "metadata": {
        "id": "etEqbszXC-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(f\"Length of nodes : {len(nodes)}\")\n",
        "print(f\"get the content for node 0 :{nodes[0].get_content(metadata_mode='all')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0x03Xp8Boi2",
        "outputId": "028daa1c-9ad2-4d57-e118-a087bca25aad"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of nodes : 43\n",
            "get the content for node 0 :page_label: 1\n",
            "file_name: self_rag_arxiv.pdf\n",
            "file_path: data/self_rag_arxiv.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 1405127\n",
            "creation_date: 2024-05-11\n",
            "last_modified_date: 2023-10-19\n",
            "\n",
            "Preprint.\n",
            "SELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\n",
            "CRITIQUE THROUGH SELF-REFLECTION\n",
            "Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§\n",
            "†University of Washington§Allen Institute for AI‡IBM Research AI\n",
            "{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\n",
            "ABSTRACT\n",
            "Despite their remarkable capabilities, large language models (LLMs) often produce\n",
            "responses containing factual inaccuracies due to their sole reliance on the paramet-\n",
            "ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\n",
            "hoc approach that augments LMs with retrieval of relevant knowledge, decreases\n",
            "such issues. However, indiscriminately retrieving and incorporating a fixed number\n",
            "of retrieved passages, regardless of whether retrieval is necessary, or passages are\n",
            "relevant, diminishes LM versatility or can lead to unhelpful response generation.\n",
            "We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\n",
            "eration ( SELF-RAG)that enhances an LM’s quality and factuality through retrieval\n",
            "and self-reflection. Our framework trains a single arbitrary LM that adaptively\n",
            "retrieves passages on-demand, and generates and reflects on retrieved passages\n",
            "and its own generations using special tokens, called reflection tokens. Generating\n",
            "reflection tokens makes the LM controllable during the inference phase, enabling it\n",
            "to tailor its behavior to diverse task requirements. Experiments show that SELF-\n",
            "RAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\n",
            "and retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\n",
            "outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\n",
            "reasoning and fact verification tasks, and it shows significant gains in improving\n",
            "factuality and citation accuracy for long-form generations relative to these models.1\n",
            "1 I NTRODUCTION\n",
            "State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\n",
            "despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n",
            "(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\n",
            "with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n",
            "2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\n",
            "unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\n",
            "retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\n",
            "the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\n",
            "the models are not explicitly trained to leverage and follow facts from provided passages. This\n",
            "work introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\n",
            "LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand\n",
            "retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\n",
            "its own generation process given a task input by generating both task output and intermittent special\n",
            "tokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\n",
            "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
            "given an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\n",
            "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\n",
            "calls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\n",
            "retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\n",
            "2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\n",
            "of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n",
            "1Our code and trained models are available at https://selfrag.github.io/ .\n",
            "1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the vectorstore"
      ],
      "metadata": {
        "id": "-6IklmYdE43y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db_mistral\")\n",
        "chroma_collection = db.get_or_create_collection(\"multidocument-agent\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "_5TyG-YnFGkm"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Embedding Model"
      ],
      "metadata": {
        "id": "6YiWDro7Ie0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n",
        "#\n",
        "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "#\n",
        "Settings.embed_model = embed_model\n",
        "#\n",
        "Settings.chunk_size = 1024\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9b2816f55c6b4f11aa72bda926291f78",
            "0e147a9ccb57409aa88c50fe44a9264a",
            "199597bbc311433ab2fee4154d1322ea",
            "115d1cd69ad04562a8b134e7359f9d43",
            "0481e925a8b740d68908b9838adbc50a",
            "5fb34447e6164acc949df4d23077b524",
            "86db8a52dcee4b84b4c274c181e17eaa",
            "3e3ea6150f064d1b9dbda0267085f2ab",
            "9fcc3d04b2fe4d74a6ebf97e53dd87c2",
            "3961d2e124c141ac95346dda98f866de",
            "2e647df92ff442c4842b784a5661f647"
          ]
        },
        "id": "xdiv_BcVIhXZ",
        "outputId": "41238baa-d8c0-420c-e9db-dd602d50ef16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b2816f55c6b4f11aa72bda926291f78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the LLM"
      ],
      "metadata": {
        "id": "vpmMIyWCFp1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from llama_index.llms.groq import Groq\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "#\n",
        "llm = Groq(model=\"gemma-7b-it\", api_key=GROQ_API_KEY)\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "2b8pbkOkFr86"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "#\n",
        "llm =  OpenAI(model=\"gpt-3.5-turbo\",api_key= userdata.get('OPENAI_API_KEY'),temperature=0)\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "AtmA05VXt3fP"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#instantiate Vectorstore\n",
        "name = \"BERT_arxiv\"\n",
        "vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
        "vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
        "#\n",
        "# Define Vectorstore Autoretrieval tool\n",
        "def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
        "  '''\n",
        "  perform vector search over index on\n",
        "  query(str): query string needs to be embedded\n",
        "  page_numbers(List[str]): list of page numbers to be retrieved,\n",
        "                          leave blank if we want to perform a vector search over all pages\n",
        "  '''\n",
        "  page_numbers = page_numbers or []\n",
        "  metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
        "  #\n",
        "  query_engine = vector_index.as_query_engine(similarity_top_k =2,\n",
        "                                              filters = MetadataFilters.from_dicts(metadata_dict,\n",
        "                                                                                    condition=FilterCondition.OR)\n",
        "                                              )\n",
        "  #\n",
        "  response = query_engine.query(query)\n",
        "  return response\n",
        "#\n",
        "#llamiondex FunctionTool wraps any python function we feed it\n",
        "vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
        "                                              fn=vector_query)\n",
        "# Prepare Summary Tool\n",
        "summary_index = SummaryIndex(nodes)\n",
        "summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\",\n",
        "                                                      se_async=True,)\n",
        "summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
        "                                                    query_engine=summary_query_engine,\n",
        "                                                  description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
        "                                              \"DO NOT USE if you have specified questions over the documents.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGhIOtuSyh3u",
        "outputId": "05123cf5-f606-4b54-909b-2a9afe228526"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 109a139c-e725-4405-8490-f3ace785d09a\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 4305b370-26b3-4a17-8df7-b192d4527107\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: dd49c984-1570-4443-878b-d5a499f0e8af\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: f11dfde6-1fd1-4602-8693-7caef87b1f01\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: f921f6c7-4260-468e-b261-82d8755bf83a\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 5c57418f-419f-4c3b-9905-0716be53620f\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 17295f5c-d02b-44bf-8ef8-f65bf80790db\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: c3206ed2-ae3e-40fa-8cc2-56f997022843\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 8282af51-715f-4409-a9d8-afb8ad7f78ad\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 6a8da67b-c194-424e-bce9-9da916034b38\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: e4ed0053-d81b-4a38-a1e6-98328848a48b\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: ed256083-cdc5-419f-94f4-9a73cc3f25d5\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bb134d33-fc37-4522-a607-4e77993a95be\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: c235275d-3555-48ee-a7f2-458a1bec83ad\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 23289079-2424-4efb-a7be-c7e03afc129d\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 5ab57571-88d5-41a9-b472-a0735f1689e8\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: f9849b30-ee01-4927-8a4d-de98d8fafdbd\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 514c88d2-ec47-46dd-9a5f-4fcc95c9e49e\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: dd39db39-29ee-4d57-91bd-50ef5da3ed7d\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: c9e317f3-c7b9-49ae-b3d6-003f6ff70821\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: e5009825-0b2d-44e1-b84b-cfbcb9fc0e3d\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 71f27a91-ec1e-479b-b2f4-4ec3fa2902a3\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 6eaf5ece-4530-45fc-933c-d33c05db24ab\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 96bf51a8-d39a-4923-ba83-9162e96dbc0a\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 66d8afab-5ed1-40a3-ac34-ecbfb564d475\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 0fe36af3-04c6-440c-b7fa-d4440f870644\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 0e6ea469-f666-49c6-98aa-4fa7c188fd80\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 33e808d7-a074-486c-878f-baa6580c8ad5\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 82feb1f0-1372-4719-b744-2f5ddbcaec9b\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: e4f2834d-5c3e-4bfa-8cfa-16d4539d84b4\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: a3344ea6-6f64-4b9b-b871-b55e71c6c700\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bb1d7f80-3e5f-41b1-b252-f3808296de6f\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: debc0ee6-6080-4ce1-9c8e-f6bc429fff31\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 25496531-97c5-4068-a5e3-cc4365c90aea\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 968f7ad1-740a-41fe-9ead-b70483c72a3b\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: d1ee604a-db30-4a4c-9210-fbed4ea9214e\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: c72687be-8d32-4473-b93c-d91f5465f55c\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: ac3dea8a-2539-4e04-8d95-119a5c22601f\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: b362fdb2-1b0c-4efd-ba62-837a19623811\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 597232c2-0732-44b6-8194-9eb0831f6beb\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 68ea04c0-cab3-4260-afa6-bdf84b7ff17b\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 5cacd224-3185-4d13-8d3b-019da9927922\n",
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: cacdf881-ca28-49f8-bf9e-7a8d63e3bb72\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 109a139c-e725-4405-8490-f3ace785d09a\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 4305b370-26b3-4a17-8df7-b192d4527107\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: dd49c984-1570-4443-878b-d5a499f0e8af\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: f11dfde6-1fd1-4602-8693-7caef87b1f01\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: f921f6c7-4260-468e-b261-82d8755bf83a\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 5c57418f-419f-4c3b-9905-0716be53620f\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 17295f5c-d02b-44bf-8ef8-f65bf80790db\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: c3206ed2-ae3e-40fa-8cc2-56f997022843\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 8282af51-715f-4409-a9d8-afb8ad7f78ad\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 6a8da67b-c194-424e-bce9-9da916034b38\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: e4ed0053-d81b-4a38-a1e6-98328848a48b\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: ed256083-cdc5-419f-94f4-9a73cc3f25d5\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: bb134d33-fc37-4522-a607-4e77993a95be\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: c235275d-3555-48ee-a7f2-458a1bec83ad\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 23289079-2424-4efb-a7be-c7e03afc129d\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 5ab57571-88d5-41a9-b472-a0735f1689e8\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: f9849b30-ee01-4927-8a4d-de98d8fafdbd\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 514c88d2-ec47-46dd-9a5f-4fcc95c9e49e\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: dd39db39-29ee-4d57-91bd-50ef5da3ed7d\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: c9e317f3-c7b9-49ae-b3d6-003f6ff70821\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: e5009825-0b2d-44e1-b84b-cfbcb9fc0e3d\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 71f27a91-ec1e-479b-b2f4-4ec3fa2902a3\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 6eaf5ece-4530-45fc-933c-d33c05db24ab\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 96bf51a8-d39a-4923-ba83-9162e96dbc0a\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 66d8afab-5ed1-40a3-ac34-ecbfb564d475\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 0fe36af3-04c6-440c-b7fa-d4440f870644\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 0e6ea469-f666-49c6-98aa-4fa7c188fd80\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 33e808d7-a074-486c-878f-baa6580c8ad5\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 82feb1f0-1372-4719-b744-2f5ddbcaec9b\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: e4f2834d-5c3e-4bfa-8cfa-16d4539d84b4\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: a3344ea6-6f64-4b9b-b871-b55e71c6c700\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: bb1d7f80-3e5f-41b1-b252-f3808296de6f\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: debc0ee6-6080-4ce1-9c8e-f6bc429fff31\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 25496531-97c5-4068-a5e3-cc4365c90aea\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 968f7ad1-740a-41fe-9ead-b70483c72a3b\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: d1ee604a-db30-4a4c-9210-fbed4ea9214e\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: c72687be-8d32-4473-b93c-d91f5465f55c\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: ac3dea8a-2539-4e04-8d95-119a5c22601f\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: b362fdb2-1b0c-4efd-ba62-837a19623811\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 597232c2-0732-44b6-8194-9eb0831f6beb\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 68ea04c0-cab3-4260-afa6-bdf84b7ff17b\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 5cacd224-3185-4d13-8d3b-019da9927922\n",
            "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: cacdf881-ca28-49f8-bf9e-7a8d63e3bb72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RLsB3dZ0my6",
        "outputId": "25968c7b-57ef-4aa3-c935-d61fee656a33"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-mistralai\n",
            "  Downloading llama_index_llms_mistralai-0.1.12-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.24 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-mistralai) (0.10.36)\n",
            "Collecting mistralai>=0.1.8 (from llama-index-llms-mistralai)\n",
            "  Downloading mistralai-0.1.8-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.14.1)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from mistralai>=0.1.8->llama-index-llms-mistralai) (3.10.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai>=0.1.8->llama-index-llms-mistralai) (2.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai>=0.1.8->llama-index-llms-mistralai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai>=0.1.8->llama-index-llms-mistralai) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-mistralai) (1.16.0)\n",
            "Installing collected packages: httpx, mistralai, llama-index-llms-mistralai\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.0\n",
            "    Uninstalling httpx-0.27.0:\n",
            "      Successfully uninstalled httpx-0.27.0\n",
            "Successfully installed httpx-0.25.2 llama-index-llms-mistralai-0.1.12 mistralai-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.mistralai import MistralAI\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get(\"MISTRAL_API_KEY\")\n",
        "llm = MistralAI(model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "id": "qWwfAjSs0uAL"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.predict_and_call([vector_query_tool],\n",
        "                                \"Summarize the content in page number 2\",\n",
        "                                verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMT2PHFKyxD2",
        "outputId": "bb12c370-15ff-4fa2-d834-557bc778e809"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Calling Function ===\n",
            "Calling function: vector_tool_BERT_arxiv with args: {\"query\": \"summarize content\", \"page_numbers\": [\"2\"]}\n",
            "=== Function Output ===\n",
            "The content discusses the use of RAG models for knowledge-intensive generation tasks, such as MS-MARCO and Jeopardy question generation, showing that the models produce more factual, specific, and diverse responses compared to a BART baseline. The models also perform well in FEVER fact verification, achieving results close to state-of-the-art pipeline models. Additionally, the models demonstrate the ability to update their knowledge as the world changes by replacing the non-parametric memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to generate Vectorstore Tool and Summary tool for all the documents"
      ],
      "metadata": {
        "id": "_MtMBmWrHGXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_tools(file_path:str,name:str)->str:\n",
        "  '''\n",
        "  get vector query and sumnmary query tools from a document\n",
        "  '''\n",
        "  #load documents\n",
        "  documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
        "  print(f\"length of nodes\")\n",
        "  splitter = SentenceSplitter(chunk_size=1024,chunk_overlap=100)\n",
        "  nodes = splitter.get_nodes_from_documents(documents)\n",
        "  print(f\"Length of nodes : {len(nodes)}\")\n",
        "  #instantiate Vectorstore\n",
        "  vector_index = VectorStoreIndex(nodes,storage_context=storage_context)\n",
        "  vector_index.storage_context.vector_store.persist(persist_path=\"/content/chroma_db\")\n",
        "  #\n",
        "  # Define Vectorstore Autoretrieval tool\n",
        "  def vector_query(query:str,page_numbers:Optional[List[str]]=None)->str:\n",
        "    '''\n",
        "    perform vector search over index on\n",
        "    query(str): query string needs to be embedded\n",
        "    page_numbers(List[str]): list of page numbers to be retrieved,\n",
        "                            leave blank if we want to perform a vector search over all pages\n",
        "    '''\n",
        "    page_numbers = page_numbers or []\n",
        "    metadata_dict = [{\"key\":'page_label',\"value\":p} for p in page_numbers]\n",
        "    #\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k =2,\n",
        "                                                filters = MetadataFilters.from_dicts(metadata_dict,\n",
        "                                                                                     condition=FilterCondition.OR)\n",
        "                                                )\n",
        "    #\n",
        "    response = query_engine.query(query)\n",
        "    return response\n",
        "  #\n",
        "  #llamiondex FunctionTool wraps any python function we feed it\n",
        "  vector_query_tool = FunctionTool.from_defaults(name=f\"vector_tool_{name}\",\n",
        "                                                fn=vector_query)\n",
        "  # Prepare Summary Tool\n",
        "  summary_index = SummaryIndex(nodes)\n",
        "  summary_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\",\n",
        "                                                       se_async=True,)\n",
        "  summary_query_tool = QueryEngineTool.from_defaults(name=f\"summary_tool_{name}\",\n",
        "                                                     query_engine=summary_query_engine,\n",
        "                                                    description=(\"Use ONLY IF you want to get a holistic summary of the documents.\"\n",
        "                                                \"DO NOT USE if you have specified questions over the documents.\"))\n",
        "  return vector_query_tool,summary_query_tool\n",
        "\n"
      ],
      "metadata": {
        "id": "1MeBwZIvHQiy"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare a input list with specified document names"
      ],
      "metadata": {
        "id": "cFPdJWLmNXz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_path = \"/content/data\"\n",
        "file_name = []\n",
        "file_path = []\n",
        "for files in os.listdir(root_path):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    file_name.append(files.split(\".\")[0])\n",
        "    file_path.append(os.path.join(root_path,file))\n",
        "#"
      ],
      "metadata": {
        "id": "GJCLcTL4Nd_T"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD4kVdkfQyuH",
        "outputId": "bf747aa9-323f-42d0-e107-30517a907251"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['self_rag_arxiv', 'crag_arxiv', 'RAG_arxiv', '', 'BERT_arxiv']"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfFYJsLxQzRK",
        "outputId": "24ac9a12-0e90-4e0e-e3ce-7acc7c6964e0"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data/BERT_arxiv.pdf',\n",
              " '/content/data/BERT_arxiv.pdf',\n",
              " '/content/data/BERT_arxiv.pdf',\n",
              " '/content/data/BERT_arxiv.pdf',\n",
              " '/content/data/BERT_arxiv.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the vectortool and summary tool for each documents"
      ],
      "metadata": {
        "id": "LYJ45zp2Q8Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers_to_tools_dict = {}\n",
        "for name,filename in zip(file_name,file_path):\n",
        "  vector_query_tool,summary_query_tool = get_doc_tools(filename,name)\n",
        "  papers_to_tools_dict[name] = [vector_query_tool,summary_query_tool]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2TOq0ceQ0xT",
        "outputId": "0386e226-e6ac-4fec-babd-72aa118df87e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of nodes\n",
            "Length of nodes : 28\n",
            "length of nodes\n",
            "Length of nodes : 28\n",
            "length of nodes\n",
            "Length of nodes : 28\n",
            "length of nodes\n",
            "Length of nodes : 28\n",
            "length of nodes\n",
            "Length of nodes : 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the tools in a flat list"
      ],
      "metadata": {
        "id": "FlA_DdqHXwm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers_to_tools_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfZjW9WwYThW",
        "outputId": "6d528091-0da8-4fe3-bc7f-3d36067732e0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'self_rag_arxiv.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7a3ea875a6b0>,\n",
              "  <llama_index.core.tools.function_tool.FunctionTool at 0x7a3ea8758490>],\n",
              " 'crag_arxiv.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7a3dca3e55a0>,\n",
              "  <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dca38b4f0>],\n",
              " 'RAG_arxiv.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7a3dca36af50>,\n",
              "  <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dc9489540>],\n",
              " 'BERT_arxiv.pdf': [<llama_index.core.tools.function_tool.FunctionTool at 0x7a3de00a85b0>,\n",
              "  <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dca36a3b0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tools = [t for f in file_name for t in papers_to_tools_dict[f]]\n",
        "initial_tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcNk5GZ-YX9c",
        "outputId": "2db9e539-0ddc-40ed-b7ab-7d7d1d2d778f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<llama_index.core.tools.function_tool.FunctionTool at 0x7a3dc973ace0>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3de01099c0>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dc946fe20>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3dc946f1f0>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dc9319600>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3de010b310>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7a3dc9394610>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3dc9394df0>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7a3eb9ed79d0>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3de2bfd000>]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform RAG over the set of tools in order to solv the issue  related to too many tool selections\n"
      ],
      "metadata": {
        "id": "EH9cTUtmajG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "#\n",
        "obj_index = ObjectIndex.from_objects(initial_tools,index_cls=VectorStoreIndex)\n",
        "#"
      ],
      "metadata": {
        "id": "bRvIHbrRbltZ"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the object index as retriever"
      ],
      "metadata": {
        "id": "J9zR1fb3dBTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=2)\n",
        "tools = obj_retriever.retrieve(\"compare and contrast the papers self rag and corrective rag\")"
      ],
      "metadata": {
        "id": "Jgm_kOmqdG9t"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZeyqp7md04R",
        "outputId": "3a475cd0-e292-40db-dd8b-8546183b9016"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<llama_index.core.tools.query_engine.QueryEngineTool at 0x7a3dc94adde0>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x7a3de010b850>]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t41DLIjVd2Jh",
        "outputId": "c323c1da-3610-4a7c-cbc2-89aaef78ab5e"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_self_rag_arxiv', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools[1].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nzwizsRd51n",
        "outputId": "8769fc10-3e0a-4058-a484-b7261ebb5979"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='vector_tool_self_rag_arxiv(query: str, page_numbers: Optional[List[str]] = None) -> str\\n\\n    perform vector search over index on\\n    query(str): query string needs to be embedded\\n    page_numbers(List[str]): list of page numbers to be retrieved,\\n                            leave blank if we want to perform a vector search over all pages\\n    ', name='vector_tool_self_rag_arxiv', fn_schema=<class 'pydantic.v1.main.vector_tool_self_rag_arxiv'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = obj_retriever.retrieve(\"Summarize the corrective rag approach.\")\n",
        "tools[1].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4jC2AGlnt5H",
        "outputId": "837d0196-5c95-496d-d8eb-18845d8e1282"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_self_rag_arxiv', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etO0A1XJn8U5",
        "outputId": "eb47a457-c5ad-43b2-e0c0-1d6d7662502c"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of the documents.DO NOT USE if you have specified questions over the documents.', name='summary_tool_RAG_arxiv', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the RAG Agent"
      ],
      "metadata": {
        "id": "3C3lKgz9eQKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner"
      ],
      "metadata": {
        "id": "UjEtuKIHeAig"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
        "                                                     llm=llm,\n",
        "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
        "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
        "                                                     verbose=True)\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "IVEEfdV5fXzT"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\"Compare and contrast self rag and crag.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldqxnmj9ltqZ",
        "outputId": "bf10f639-69f5-4b52-8b55-82dfd9c3ec40"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Compare and contrast self rag and crag.\n",
            "=== LLM Response ===\n",
            "Sure, I'd be happy to help you understand the differences between Self RAG and CRAG, based on the functions provided to me.\n",
            "\n",
            "Self RAG (Retrieval-Augmented Generation) is a method where the model generates a holistic summary of the documents provided as input. It's important to note that this method should only be used if you want a general summary of the documents, and not if you have specific questions over the documents.\n",
            "\n",
            "On the other hand, CRAG (Contrastive Retrieval-Augmented Generation) is also a method for generating a holistic summary of the documents. The key difference between CRAG and Self RAG is not explicitly clear from the functions provided. However, the name suggests that CRAG might use a contrastive approach in its retrieval process, which could potentially lead to a summary that highlights the differences and similarities between the documents more effectively.\n",
            "\n",
            "Again, it's crucial to remember that both of these methods should only be used for a holistic summary, and not for answering specific questions over the documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILlFdjSI25hz",
        "outputId": "71f7f62b-3543-4811-f749-871062bb42d8"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Sure, I'd be happy to help you understand the differences between Self RAG and CRAG, based on the functions provided to me.\n",
            "\n",
            "Self RAG (Retrieval-Augmented Generation) is a method where the model generates a holistic summary of the documents provided as input. It's important to note that this method should only be used if you want a general summary of the documents, and not if you have specific questions over the documents.\n",
            "\n",
            "On the other hand, CRAG (Contrastive Retrieval-Augmented Generation) is also a method for generating a holistic summary of the documents. The key difference between CRAG and Self RAG is not explicitly clear from the functions provided. However, the name suggests that CRAG might use a contrastive approach in its retrieval process, which could potentially lead to a summary that highlights the differences and similarities between the documents more effectively.\n",
            "\n",
            "Again, it's crucial to remember that both of these methods should only be used for a holistic summary, and not for answering specific questions over the documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using OpenAI as LLM"
      ],
      "metadata": {
        "id": "lPCqqgW93BP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever,\n",
        "                                                     llm=llm,\n",
        "                                                     system_prompt=\"\"\"You are an agent designed to answer queries over a set of given papers.\n",
        "                                                     Please always use the tools provided to answer a question.Do not rely on prior knowledge.\"\"\",\n",
        "                                                     verbose=True)\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "DMrKh45t28Yc"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\"Summarize the paper corrective RAG.\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkvElSFb3QER",
        "outputId": "c2446e14-fa81-4b53-e651-6681f887b20c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Summarize the paper corrective RAG.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_RAG_arxiv with args: {\"input\": \"corrective RAG\"}\n",
            "=== Function Output ===\n",
            "The corrective RAG approach is a method used to address issues or errors in a system by categorizing them into three levels: Red, Amber, and Green. Red signifies critical problems that need immediate attention, Amber indicates issues that require monitoring or action in the near future, and Green represents no significant concerns. This approach helps prioritize and manage corrective actions effectively based on the severity of the identified issues.\n",
            "=== LLM Response ===\n",
            "The corrective RAG approach categorizes issues into Red, Amber, and Green levels to prioritize and manage corrective actions effectively based on severity. Red signifies critical problems needing immediate attention, Amber requires monitoring or action soon, and Green indicates no significant concerns.\n",
            "assistant: The corrective RAG approach categorizes issues into Red, Amber, and Green levels to prioritize and manage corrective actions effectively based on severity. Red signifies critical problems needing immediate attention, Amber requires monitoring or action soon, and Green indicates no significant concerns.\n"
          ]
        }
      ]
    }
  ]
}